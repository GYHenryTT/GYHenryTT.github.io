<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Naive Bayes Classifier</title>
    <url>/2020/03/01/Naive-Bayes-Classifier/</url>
    <content><![CDATA[<div align="center">

<h1 id="Naive-Bayes-Classifier-and-Evaluation"><a href="#Naive-Bayes-Classifier-and-Evaluation" class="headerlink" title="Naive Bayes Classifier and Evaluation"></a>Naive Bayes Classifier and Evaluation</h1></div>

<h2 id="Program-Instruction"><a href="#Program-Instruction" class="headerlink" title="Program Instruction"></a>Program Instruction</h2><p>The source code has been posted in my <a href="https://github.com/GYHenryTT/Computational-Linguistic/tree/master/PA2" target="_blank" rel="noopener">github</a></p>
<p>Basic Naive Bayes function: $$P(C|w)=\frac{P(W|C)P(C)}{P(W)}$$<br>$$P(C|W) \propto P(W|C)P(C),$$</p>
<p>where $P(W|C)$ is called as likelihood and  $P(C)$ as prior.</p>
<h3 id="1-Training-Function"><a href="#1-Training-Function" class="headerlink" title="1. Training Function"></a>1. Training Function</h3><p>To collect the word counts, we need to first split the text in the training set. In this case, I used regular expression <strong>‘[^A-Za-z\‘]+’</strong> as the seperator. Specifically, it will discard all non-letters except for single quote because some word with quote may have different meanings such as “I’m”, “That’s”, etc.</p>
<p>Then, the class initialization was changed. I used set to collect all the features, and use variable <em>class_count</em> to collect class numbers. Inside the train function, it can visit every seperate word and generate word counts based on the document’s class. Accordingly, it will caculate the priors and likelihoods.</p>
<a id="more"></a>

<h3 id="2-Evaluation-Function"><a href="#2-Evaluation-Function" class="headerlink" title="2. Evaluation Function"></a>2. Evaluation Function</h3><p>This function will generate precision, recall, f1 score and accuracy for both positive class and negative class. If the mode is ‘print’, the function will print out all the results based on the class, otherwise the function will return f1 score for both class as well as accuracy for further analysis (in this case, for grid search, which will be mentioned latter).</p>
<h3 id="3-Feature-selection"><a href="#3-Feature-selection" class="headerlink" title="3. Feature selection"></a>3. Feature selection</h3><p>I applied the mutual information to select features. By this function, I calculated mutual information (also called information gain) for each word in the training set. Afterwards, the function will choose a specific number of the words with top mutual infomation. The number of choosing words would be based on user-defined selection rate (a ratio from 0 to 1).</p>
<h3 id="4-Grid-Search-Function"><a href="#4-Grid-Search-Function" class="headerlink" title="4. Grid Search Function"></a>4. Grid Search Function</h3><p>This is an add-on function for selecting the best feature select rate. This function will take a list of select rates as input. It will use the selected feature to train and test model, print and plot the evaluation results.</p>
<h2 id="Result-Analysis"><a href="#Result-Analysis" class="headerlink" title="Result Analysis"></a>Result Analysis</h2><h3 id="1-Raw-Result"><a href="#1-Raw-Result" class="headerlink" title="1. Raw Result"></a>1. Raw Result</h3><p>At the beginning, I trained the model without feature selection (you may set selection rate as 0 to verify the result). The evaluation result is as followed.</p>
<p><img src="/2020/03/01/Naive-Bayes-Classifier/rawresult.png" alt="avatar"></p>
<p>The raw results are not satisfying, which is why we need to do feature selection.</p>
<h3 id="2-Adjustment-and-Optimization"><a href="#2-Adjustment-and-Optimization" class="headerlink" title="2. Adjustment and Optimization"></a>2. Adjustment and Optimization</h3><p>Then, I implement grid search to find the best select rate and get the following results. These two plots have select rate as X-axis and evaluation result as Y-axis (f1 in both negative and positive class as well as accuracy)</p>
<p><img src="/2020/03/01/Naive-Bayes-Classifier/plot_1.png" alt="avatar"><br><img src="/2020/03/01/Naive-Bayes-Classifier/plot2.png" alt="avatar"></p>
<p>Select rates in the first plot are from 0.01 to 0.99 with step size 0.05, where I found the model perform well in small selection rate. So in the second plot, I chose select rate from 0.001 to 0.1 with step size 0.001. Finally, I got the best selection rate 0.026 which means there will be 1010 words in the selected features. The evaluation of my Naive Bayes model with the best feature set is shown below.</p>
<p><img src="/2020/03/01/Naive-Bayes-Classifier/finalresult.png" alt="avatar"></p>
<p>F1 score for each class and the accuracy have increased by 10 percent based on this feature set. </p>
]]></content>
      <categories>
        <category>Project</category>
        <category>Computational Linguistics</category>
      </categories>
      <tags>
        <tag>Naive Bayes</tag>
        <tag>NLP</tag>
        <tag>Computational Linguistics</tag>
      </tags>
  </entry>
  <entry>
    <title>Naive Bayes and Logistic Regression</title>
    <url>/2020/02/20/Naive-Bayes-and-Logistic-Regression/</url>
    <content><![CDATA[<div align="center">

<h1 id="Naive-Bayes-and-Logistic-Regression-in-Text-Classification"><a href="#Naive-Bayes-and-Logistic-Regression-in-Text-Classification" class="headerlink" title="Naive Bayes and Logistic Regression in Text Classification"></a>Naive Bayes and Logistic Regression in Text Classification</h1></div>

<h2 id="Simple-Practice"><a href="#Simple-Practice" class="headerlink" title="Simple Practice"></a>Simple Practice</h2><p>Assume we have a document, Given the following short movie reviews, each labeled with a genre, either comedy or action:</p>
<table>
<thead>
<tr>
<th align="center">document</th>
<th align="center">class</th>
</tr>
</thead>
<tbody><tr>
<td align="center">ﬂy fast shoot love</td>
<td align="center">action</td>
</tr>
<tr>
<td align="center">fun couple love love</td>
<td align="center">comedy</td>
</tr>
<tr>
<td align="center">fast furious shoot</td>
<td align="center">action</td>
</tr>
<tr>
<td align="center">couple ﬂy fast fun fun</td>
<td align="center">comedy</td>
</tr>
<tr>
<td align="center">furious shoot shoot fun</td>
<td align="center">action</td>
</tr>
</tbody></table>
<p>and a new document D: [fast couple shoot ﬂy]</p>
<p>compute the most likely class for D.</p>
<a id="more"></a>

<h3 id="1-Naive-Bayes"><a href="#1-Naive-Bayes" class="headerlink" title="1. Naive Bayes"></a>1. Naive Bayes</h3><p>The generative model for Naive Bayes is $$P(class|feature) = \frac{P(feature|class)P(class)}{P(feature)}$$</p>
<p>Based on the independence assumptions, we will have a caculation formula for the new document “fast couple shoot fly” as $$P(class|feature) \propto P(fast|class)P(couple|class)P(shoot|class)P(fly|class)P(class)$$My calculations for the value on the right side are as followed.</p>
<p>$P(comedy)=\frac{n(comedy)}{n(all\ class)}=\frac{2}{5}$<br>$P(action)=\frac{3}{5}$</p>
<p>While the bag of words would be {fly, fast, shoot, love, fun, couple, fast, furious} with length $d=7$, using add-1 smoothing for the likelihoods, I got a conditional probability table,</p>
<table>
<thead>
<tr>
<th align="center">P(feature|class)</th>
<th align="right">comedy</th>
<th align="right">action</th>
</tr>
</thead>
<tbody><tr>
<td align="center">fast</td>
<td align="right">$\frac{n(fast)+1}{words(comedy)+d}=\frac{1+1}{9+7}=\frac{1}{8}$</td>
<td align="right">$\frac{n(fast)+1}{words(action)+d}=\frac{2+1}{11+7}=\frac16$</td>
</tr>
<tr>
<td align="center">couple</td>
<td align="right">$ \frac 3{16}$</td>
<td align="right">$ \frac1{18} $</td>
</tr>
<tr>
<td align="center">shoot</td>
<td align="right">$ \frac1{16}$</td>
<td align="right">$ \frac5{18} $</td>
</tr>
<tr>
<td align="center">fly</td>
<td align="right">$\frac1{8}$</td>
<td align="right">$\frac1{9}$</td>
</tr>
</tbody></table>
<p>Therefore, with the formula mentioned before, the probability of document D belonging to each class will be (use log probability),</p>
<p>$\log{P(comedy|D)} \propto \log(\frac{1}{8} \times \frac{3}{16} \times\frac{1}{16} \times\frac{1}{8} \times \frac{2}{5}) \approx-9.522$<br>$\log{P(action|D)} \propto -8.680 &gt; -9.764$</p>
<p>As a result, the most likely class for D is <strong>action</strong>.</p>
<h3 id="2-Logistic-Regression"><a href="#2-Logistic-Regression" class="headerlink" title="2. Logistic Regression"></a>2. Logistic Regression</h3><h4 id="2-1-Process-the-first-mini-batch"><a href="#2-1-Process-the-first-mini-batch" class="headerlink" title="2.1 Process the first mini-batch"></a>2.1 Process the first mini-batch</h4><p>Using cross entropy lost function<br>$$L=y\log{\hat{y}} + (1-y)\log{(1-\hat{y})}$$ in this case,  $\hat{y}=\sigma(\theta^TX)$, $\theta=[w_{fast}, w_{couple}, w_{shoot}, w_{fly}, b]$ and $\sigma(z)=\frac1{1+e^{-z}}$</p>
<p>we will get a general gradient formula as<br>$$<br>\nabla L = \begin{bmatrix}<br>[\sigma(\theta^TX)-y]x_{fast}\\<br>[\sigma(\theta^TX)-y]x_{couple}\\<br>[\sigma(\theta^TX)-y]x_{shoot}\\<br>[\sigma(\theta^TX)-y]x_{fly}\\<br>\sigma(\theta^TX)-y<br>\end{bmatrix}<br>$$<br>So the individual gradiant for the first three examples will be<br>$$<br>\nabla L_{1} = \begin{bmatrix}<br>(0.5 - 0)\times1\\<br>(0.5 - 0)\times0\\<br>(0.5 - 0)\times1\\<br>(0.5 - 0)\times1\\<br>0.5 - 0<br>\end{bmatrix} =<br>\begin{bmatrix}<br>0.5\\<br>0\\<br>0.5\\<br>0.5\\<br>0.5<br>\end{bmatrix}<br>$$<br>$$<br>\nabla L_{2} =<br>\begin{bmatrix}<br>0\\<br>-0.5\\<br>0\\<br>0\\<br>-0.5<br>\end{bmatrix}<br>$$<br>$$<br>\nabla L_{3} =<br>\begin{bmatrix}<br>0.5\\<br>0\\<br>0.5\\<br>0\\<br>0.5<br>\end{bmatrix}<br>$$</p>
<p>The overall gradiant will be</p>
<p>$$<br>\nabla L_{firstbatch} =<br>\begin{bmatrix}<br>\frac13\\<br>-\frac16\\<br>\frac13\\<br>\frac16\\<br>\frac16<br>\end{bmatrix}<br>$$</p>
<p>Use the gradiant to update the weight vector with learning rate $\eta=0.1$</p>
<p>$$<br>\theta_{1} = \theta_{0}-\eta\nabla L_{firstbatch}=<br>\begin{bmatrix}<br>-\frac1{30}\\<br>\frac1{60}\\<br>-\frac1{30}\\<br>-\frac1{60}\\<br>-\frac1{60}<br>\end{bmatrix}<br>$$</p>
<h4 id="2-2-Process-the-second-mini-batch"><a href="#2-2-Process-the-second-mini-batch" class="headerlink" title="2.2 Process the second mini-batch"></a>2.2 Process the second mini-batch</h4><p>Caculate individual gradiant in second batch, where $\sigma(\theta_1^TX)=\sigma(1\times(−0.0333) + 1\times(0.0167) + 0\times(−0.0333) + 1\times(−0.0167) − 0.0167)=\sigma(-0.05)=0.4875$</p>
<p>$$<br>\nabla L_{4} =<br>\begin{bmatrix}<br>[\sigma(\theta_1^TX)-1]\times1\\<br>[\sigma(\theta_1^TX)-1]\times1\\<br>[\sigma(\theta_1^TX)-1]\times0\\<br>[\sigma(\theta_1^TX)-1]\times1\\<br>\sigma(\theta_1^TX)-1<br>\end{bmatrix}=<br>\begin{bmatrix}<br>−0.5125\\<br>−0.5125\\<br>0\\<br>−0.5125\\<br>−0.5125<br>\end{bmatrix}<br>$$<br>$$<br>\nabla L_{5} =<br>\begin{bmatrix}<br>[\sigma(\theta_1^TX)-0]\times0\\<br>[\sigma(\theta_1^TX)-0]\times0\\<br>[\sigma(\theta_1^TX)-0]\times2\\<br>[\sigma(\theta_1^TX)-0]\times0\\<br>\sigma(\theta_1^TX)-0<br>\end{bmatrix}=<br>\begin{bmatrix}<br>0\\<br>0\\<br>0.9584\\<br>0\\<br>0.4792<br>\end{bmatrix}<br>$$</p>
<p>The overall gradiant for second batch will be</p>
<p>$$<br>\nabla L_{secondbatch} =<br>\begin{bmatrix}<br>-0.2562\\<br>-0.2562\\<br>0.4792\\<br>-0.2562\\<br>-0.0167<br>\end{bmatrix}<br>$$</p>
<p>Use this gradiant to update the weight vector with learning rate $\eta=0.1$</p>
<p>$$<br>\begin{aligned}<br>\theta_{2} &amp;= \theta_{1}-\eta\nabla L_{secondbatch}\\<br>&amp;=<br>\begin{bmatrix}<br>-\frac1{30}\\<br>\frac1{60}\\<br>-\frac1{30}\\<br>-\frac1{60}\\<br>-\frac1{60}<br>\end{bmatrix}-0.1<br>\begin{bmatrix}<br>-0.2562\\<br>-0.2562\\<br>0.4792\\<br>-0.2562\\<br>-0.0167<br>\end{bmatrix}\\<br>&amp;=<br>\begin{bmatrix}<br>-0.0077\\<br>0.0423\\<br>-0.0813\\<br>0.0090\\<br>-0.0150<br>\end{bmatrix}<br>\end{aligned}<br>$$</p>
<h3 id="3-Compute-the-probability"><a href="#3-Compute-the-probability" class="headerlink" title="3. Compute the probability"></a>3. Compute the probability</h3><p>The probabability that D has the class “comedy” will be<br>$$\begin{aligned}<br>P(y=1|D)&amp;=\sigma(\theta^TX)\\<br>&amp;=\sigma{(\begin{bmatrix}<br>-0.0077\\<br>0.0423\\<br>-0.0813\\<br>0.0090\\<br>-0.0150<br>\end{bmatrix}^T<br>\begin{bmatrix}<br>1\\<br>1\\<br>1\\<br>1\\<br>1<br>\end{bmatrix})}\\<br>&amp;=0.4868<br>\end{aligned}$$</p>
<p>Therefore, the most likely class for D is <strong>action</strong>.</p>
]]></content>
      <categories>
        <category>Learning Journal</category>
        <category>Computational Linguistics</category>
      </categories>
      <tags>
        <tag>Naive Bayes</tag>
        <tag>NLP</tag>
        <tag>Computational Linguistics</tag>
        <tag>Logistic Regression</tag>
        <tag>Gradient Descent</tag>
      </tags>
  </entry>
</search>
