<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Neural Network Parsing</title>
    <url>/2020/05/12/Neural-Network-Parsing/</url>
    <content><![CDATA[<div align="center">

<h1 id="Neural-Transition-Based-Dependency-Parsing"><a href="#Neural-Transition-Based-Dependency-Parsing" class="headerlink" title="Neural Transition-Based Dependency Parsing"></a>Neural Transition-Based Dependency Parsing</h1></div>

<h2 id="Worked-Example"><a href="#Worked-Example" class="headerlink" title="Worked Example"></a>Worked Example</h2><h3 id="1-Go-through-the-sequence-of-transitions"><a href="#1-Go-through-the-sequence-of-transitions" class="headerlink" title="1. Go through the sequence of transitions"></a>1. Go through the sequence of transitions</h3><p>Given the sentence:</p>
<pre><code>I parsed this sentence correctly</code></pre><table>
<thead>
<tr>
<th align="left">Stack</th>
<th align="left">Buffer</th>
<th align="left">New dependency</th>
<th align="left">Transition</th>
</tr>
</thead>
<tbody><tr>
<td align="left">[ROOT]</td>
<td align="left">[I, parsed, this, sentence, correctly]</td>
<td align="left"></td>
<td align="left">Initial Configuration</td>
</tr>
<tr>
<td align="left">[ROOT, I]</td>
<td align="left">[parsed, this, sentence, correctly]</td>
<td align="left"></td>
<td align="left">SHIFT</td>
</tr>
<tr>
<td align="left">[ROOT, I, parsed]</td>
<td align="left">[this, sentence, correctly]</td>
<td align="left"></td>
<td align="left">SHIFT</td>
</tr>
<tr>
<td align="left">[ROOT, parsed]</td>
<td align="left">[this, sentence, correctly]</td>
<td align="left">parsed $\rightarrow$ I</td>
<td align="left">LEFT-ARC</td>
</tr>
<tr>
<td align="left">[ROOT, parsed, this]</td>
<td align="left">[sentence, correctly]</td>
<td align="left"></td>
<td align="left">SHIFT</td>
</tr>
<tr>
<td align="left">[ROOT, parsed, this, sentence]</td>
<td align="left">[correctly]</td>
<td align="left"></td>
<td align="left">SHIFT</td>
</tr>
<tr>
<td align="left">[ROOT, parsed, sentence]</td>
<td align="left">[correctly]</td>
<td align="left">sentence $\rightarrow$ this</td>
<td align="left">LEFT-ARC</td>
</tr>
<tr>
<td align="left">[ROOT, parsed]</td>
<td align="left">[correctly]</td>
<td align="left">parsed $\rightarrow$ sentence</td>
<td align="left">RIGHT-ARC</td>
</tr>
<tr>
<td align="left">[ROOT, parsed, correctly]</td>
<td align="left">[]</td>
<td align="left"></td>
<td align="left">SHIFT</td>
</tr>
<tr>
<td align="left">[ROOT, parsed]</td>
<td align="left">[]</td>
<td align="left">parsed $\rightarrow$ correctly</td>
<td align="left">RIGHT-ARC</td>
</tr>
<tr>
<td align="left">[ROOT]</td>
<td align="left">[]</td>
<td align="left">ROOT $\rightarrow$ parsed</td>
<td align="left">RIGHT-ARC</td>
</tr>
</tbody></table>
<h3 id="2-A-sentence-containing-n-words-will-be-parsed-in-how-many-steps"><a href="#2-A-sentence-containing-n-words-will-be-parsed-in-how-many-steps" class="headerlink" title="2. A sentence containing $n$ words will be parsed in how many steps"></a>2. A sentence containing $n$ words will be parsed in how many steps</h3><p>For each word of the sentence, it need first be shifted onto the stack and then reduced by right\left arc. Therefore, there would be $2*n$ parsing steps for a sentence containing $n$ words regardless of the initial configuration.</p>
<h3 id="Check-Dependencies-Error"><a href="#Check-Dependencies-Error" class="headerlink" title="Check Dependencies Error"></a>Check Dependencies Error</h3><p>Dependencies Error:</p>
<pre><code>Prepositional Phrase Attachment Error
Verb Phrase Attachment Error
Modiﬁer Attachment Error
Coordination Attachment Error</code></pre><p>Given four sentences, each one has one dependency error from above. </p>
<p><img src="/2020/05/12/Neural-Network-Parsing/s1.png" alt="avatar"></p>
<p>Error type: Verb Phrase Attachment Error<br>Incorrect dependency: wedding $\rightarrow$ fearing<br>Correct dependency: heading $\rightarrow$ fearing</p>
<p><img src="/2020/05/12/Neural-Network-Parsing/s2.png" alt="avatar"></p>
<p>Error type: Coordination Attachment Error<br>Incorrect dependency: makes $\rightarrow$ rescue<br>Correct dependency: rush $\rightarrow$ rescue</p>
<p><img src="/2020/05/12/Neural-Network-Parsing/s3.png" alt="avatar"></p>
<p>Error type: Prepositional Phrase Attachment Error<br>Incorrect dependency: named $\rightarrow$ Midland<br>Correct dependency: guy $\rightarrow$ Midland</p>
<p><img src="/2020/05/12/Neural-Network-Parsing/s4.png" alt="avatar"></p>
<p>Error type: Modiﬁer Attachment Error<br>Incorrect dependency: elements $\rightarrow$ most<br>Correct dependency: crucial $\rightarrow$ most</p>
<h2 id="Neural-Network-Program"><a href="#Neural-Network-Program" class="headerlink" title="Neural Network Program"></a>Neural Network Program</h2><h3 id="Instruction"><a href="#Instruction" class="headerlink" title="Instruction"></a>Instruction</h3><p>In <em>parser_model.py</em>, weight matrices contain the weight of bias term, so the shape of which is added 1 on rows (first dimension). Also, of the outputs from <em>forward</em> function, the last column (second dimension) of hidden layer outputs is filled with ones representing bias term. </p>
<p>In terms of <em>train_for_epoch</em> in <em>run.py</em>, I take advantage of numpy broadcasting. For each sentence, the backpropagation for all layers is $\nabla_lL=x_l\odot\delta_l$, since we are applying the sum of gradients from one minibatch, my code alters this formula into $\nabla_lL=X_l^T\cdot\Delta_l$, where $X_l$ is the input matrix of current layer with shape of (batch size, input features) and $\Delta_l$ is the error term matrix with shape (batch size, output features). The dot product would be the summation of gradients aggregating by batch size.</p>
<h3 id="Model-Training-and-Evaluation"><a href="#Model-Training-and-Evaluation" class="headerlink" title="Model Training and Evaluation"></a>Model Training and Evaluation</h3><p>With the default hyperparameters (hidden_size=200, lr=0.0005, epoch=10), I got the following results:</p>
<p><img src="/2020/05/12/Neural-Network-Parsing/raw_test.png" alt="avatar"></p>
<p>Then, I implemented a simple grid search on debug mode. First, I changed learning rate to 0.001 to achieve a quick convegence. Accordingly, with differents set of hidden_size and epoch, the grid search results are as bellow:</p>
<table>
<thead>
<tr>
<th>hidden_size/epoch</th>
<th>100</th>
<th>150</th>
<th>200</th>
<th>250</th>
<th>300</th>
<th>400</th>
</tr>
</thead>
<tbody><tr>
<td>10</td>
<td>0.238 <br> 62.22</td>
<td>0.215 <br> 68.63</td>
<td>0.232 <br> 66.95</td>
<td>0.271 <br> 59.91</td>
<td>0.220 <br> 65.02</td>
<td>0.227 <br> 65.24</td>
</tr>
<tr>
<td>20</td>
<td>0.213 <br> 67.21</td>
<td>0.144 <br> 68.84</td>
<td>0.279 <br> 65.99</td>
<td>0.190 <br> 57.46</td>
<td>0.182 <br> 66.09</td>
<td>0.120 <br> 73.86</td>
</tr>
<tr>
<td>30</td>
<td>0.136 <br> 70.85</td>
<td>0.121 <br> 73.91</td>
<td>0.132 <br> 74.96</td>
<td>0.150 <br> 70.57</td>
<td>0.123 <br> 67.50</td>
<td>0.132 <br> 73.04</td>
</tr>
</tbody></table>
<p>The best parameter set is epoch=30, hidden_size=200. After about 20 epoches, the loss and UAS did not change a lot and the result is not much better than 10 epoch</p>
<p><img src="/2020/05/12/Neural-Network-Parsing/result_1.png" alt="avatar"></p>
]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>Computational Linguistics</tag>
        <tag>Neural Network</tag>
        <tag>Dependency Parsing</tag>
      </tags>
  </entry>
  <entry>
    <title>Distributional Semantics</title>
    <url>/2020/04/27/Distributional-Semantics/</url>
    <content><![CDATA[<div align="center">

<h1 id="Distributional-Semantics-Takes-the-SAT"><a href="#Distributional-Semantics-Takes-the-SAT" class="headerlink" title="Distributional Semantics Takes the SAT"></a>Distributional Semantics Takes the SAT</h1></div>

<h2 id="1-Create-distributional-semantic-word-vectors"><a href="#1-Create-distributional-semantic-word-vectors" class="headerlink" title="1 Create distributional semantic word vectors"></a>1 Create distributional semantic word vectors</h2><h3 id="Program-Introduction"><a href="#Program-Introduction" class="headerlink" title="Program Introduction"></a>Program Introduction</h3><p>Program details can be seen in Section_1.py. </p>
<p>In this section, I created a class <em>Embedding</em> to complete all the functions needed where <em>word_to_vector</em> function can generate the co-occurrence matrix and PPMI matrix based on the small artificial corpus. Additionally, function <em>get_word_vector</em> is used to print both the count vector and PPMI weighted vector according to the word input. Function <em>euclidean_distance</em> can compute euclidean distance between any two words in the corpus. </p>
<h3 id="Compare-the-word-vector-before-and-after-PPMI-reweighting"><a href="#Compare-the-word-vector-before-and-after-PPMI-reweighting" class="headerlink" title="Compare the word vector before and after PPMI reweighting"></a>Compare the word vector before and after PPMI reweighting</h3><p>The difference between original count vector and PPMI weighted vector for ‘dogs’ is shown below:</p>
<p><img src="/2020/04/27/Distributional-Semantics/dogs_vector_1.png" alt="avatar"></p>
<p>As we can seen in this result, PPMI set 0 value in context words including ‘dogs’, ‘feed’, ‘like’, ‘men’ and ‘women’. These are consistent with count vector (mostly count for 1) except for ‘like’ (count for 11), which makes sense because we are using co-occurrance based on bigrams and from the corpus we could not see many “(‘dogs’, ‘like’)”s. </p>
<p>Most importantly, it is very excitiing to see that word like ‘the’ has the less value (0.87) in PPMI vector even though with very high word counts (91) comparing to ‘bite’ (1.65), because ‘the’ usually could not provide much information in normal corpus. Overall, the PPMI vector is intuitively better than the original count vector by outstanding the obviously related word ‘bite’. </p>
<h3 id="Euclidean-Distance"><a href="#Euclidean-Distance" class="headerlink" title="Euclidean Distance"></a>Euclidean Distance</h3><p>Compute the distance between the following word pairs, note that here I applied PPMI word vector:</p>
<pre><code>“women” and “men” (human noun vs. human noun)
“women” and “dogs” (human noun vs. animal noun)
“men” and “dogs” (human noun vs. animal noun)
“feed” and “like” (human verb vs. human verb)
“feed” and “bite” (human verb vs. animal verb)
“like” and “bite” (human verb vs. animal verb)</code></pre><p>The results is as followed:</p>
<p><img src="/2020/04/27/Distributional-Semantics/eu_distance.png" alt="avatar"></p>
<p>From the table, similar word pairs tend to have less euclidean distance. For example, ‘women’ and ‘men’ has a short distance of 0.68 comparing to ‘men’ and ‘dogs’ with the distance of 2.02. This result confirms our intuition from distributional semantics.</p>
<h3 id="Singular-value-decomposition-SVD"><a href="#Singular-value-decomposition-SVD" class="headerlink" title="Singular-value decomposition (SVD)"></a>Singular-value decomposition (SVD)</h3><p>The following results are PPMI original matrix and recovered matrix, where the two matrix share the same value except for zero values which may be due to the precistion loss of floating-point.</p>
<p><img src="/2020/04/27/Distributional-Semantics/SVD.png" alt="avatar"></p>
<h3 id="Reduced-PPMI-matrix"><a href="#Reduced-PPMI-matrix" class="headerlink" title="Reduced PPMI matrix"></a>Reduced PPMI matrix</h3><p>After reducing the dimensions to 3 on PPMI matrix, I calculated a new euclidean distance table (shown below):</p>
<p><img src="/2020/04/27/Distributional-Semantics/eu_distance_2.png" alt="avatar"></p>
<p>The result keeps the information we need for each word vector because it still confirms our intuition that similar words appear in similar contexts and they have shorter euclidean distance between each other.</p>
<h2 id="2-Computing-with-distributional-semantic-word-vectors"><a href="#2-Computing-with-distributional-semantic-word-vectors" class="headerlink" title="2 Computing with distributional semantic word vectors"></a>2 Computing with distributional semantic word vectors</h2><h3 id="Programming"><a href="#Programming" class="headerlink" title="Programming"></a>Programming</h3><p>Program details can be seen in Section_2.py.</p>
<p>Function <em>create_test_set</em> randomly chose 1000 line from synonymous verbs with replacement, and for each verb, picked 4 random non-synonyms from the data set. Also, set the data set as the answers for multiple-choice questions. This function only ran once since I stored the test set and corrected answers as csv files (test_set.csv, test_correct.csv). Therefore, for the rest of the code, you could simply run <em>load_test_set</em> function to repeat the results. Note that there may be some unknown word in the test set, <em>get_word_vector</em> function could deal with the unknown word by making it a zero array, because the unknown words bear no context from training set so that we could assume they are unrelated to other words. Here I add a really small value (1e-16) on the unknown word vector to avoid error in calculating the cosine similarity. Accordingly, this function will return the word vector. </p>
<p><em>test</em> function collects the accuate numbers of synonymous word predictions including two method, euclidean distance and cosine similarity. To simplify my code, I used cosine distance to represent similarity (cosine distance = 1 - cosine similarity). The prediction word is the one having shortest distance to the base word. </p>
<p>In order to solve SAT Analogy questions, I created a new class inheriting <em>Embedding</em>. I rewrote the <em>create_test_set</em> and <em>test</em> functions to manage the new data set and added a new function <em>create_vector</em> which can help me conveniently create new vectors by any method function.</p>
<h3 id="Synonym-detection"><a href="#Synonym-detection" class="headerlink" title="Synonym detection"></a>Synonym detection</h3><p>The following table contains the accuracy among different set of word vectors and different distance methods:</p>
<p><img src="/2020/04/27/Distributional-Semantics/synonym_test.png" alt="avatar"></p>
<p>It is obvious that word2vec set performs better than COMPOSES and cosine similarity method lead to a higher accuracy. </p>
<h3 id="SAT-Analogy"><a href="#SAT-Analogy" class="headerlink" title="SAT Analogy"></a>SAT Analogy</h3><p>In this case, we need to figure out which word pair is the best match to the based word pair, easy to think about calculating the distance between each word pair and find out the closest one to the distance value of based pair. Intuitively it comes to my mind that I could continue applying cosine distance method based on the word2vec set of word vectors since it performs well in synonym detection. However, after checking the data set, I found cosine similarity may not be applied directly here because it ignores the direction between the two words. For example, in the second question from the data set (listed below), I believed “ostrich bird” and “primate monkey” shared similar cosine distance but it is obviously a wrong answer.</p>
<pre><code>ostrich bird n:n
    lion cat n:n
    goose flock n:n
    ewe sheep n:n
    cub bear n:n
    primate monkey n:n</code></pre><p>So, the direction between two word vectors is important here. Accordingly, I consider the subtraction of two word vectors because it indicates the change (direction) between two vectors explaned by the following figure. After generating this new subtraction vector, I could still use cosine similarity to figure out how close among those new vetors.</p>
<p><img src="/2020/04/27/Distributional-Semantics/subtraction.jpg" alt="avatar"></p>
<p>Therefore, I created subtracted new vector and surprisingly got a high accuracy at 0.438. This exciting finding encouraged me to apply more methods on creating new vectors, such as adding, multiplying, concatenating. And I gained the following results:</p>
<p><img src="/2020/04/27/Distributional-Semantics/analogy.png" alt="avatar"></p>
<p>First of all, these methods all have significant higher accuracy than the random guessing (20%). Another interesting part is that concatenating method also brings a high accuracy (0.422). The reason I think may be that concatenating vectors contain all the information from the paired vectors, concatenating does not change anything on the two vectors, the advantage of which is protecting the information but at the same time the data is kind of redundant. In my opinion, the concatenating method is a good baseline model in analogy. Comparing to concatenating, subtracting, adding and multiplying methods all convert the information to a new form, which intuitively thinking may cause information loss. However, subtraction decribes the direction between two vectors. Because it emphasizes this essential part though losing some other information, we could expect a higher accuracy with this method. The performance of the rest two methods, adding and multiplying, are not very saticfying. This phenomenon is similar to feature reduction or engineering, new information from these methods is not enough to make up for lost information. </p>
]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>Computational Linguistics</tag>
        <tag>Co-occurrence Matrix</tag>
        <tag>PPMI Matrix</tag>
        <tag>Synonym Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Hidden Markov</title>
    <url>/2020/04/10/Hidden-Markov/</url>
    <content><![CDATA[<div align="center">

<h1 id="Part-of-speech-Tagging-with-Hidden-Markov-Models"><a href="#Part-of-speech-Tagging-with-Hidden-Markov-Models" class="headerlink" title="Part-of-speech Tagging with Hidden Markov Models"></a>Part-of-speech Tagging with Hidden Markov Models</h1></div>

<h2 id="Program-Instruction"><a href="#Program-Instruction" class="headerlink" title="Program Instruction"></a>Program Instruction</h2><h3 id="Training-function"><a href="#Training-function" class="headerlink" title="Training function"></a>Training function</h3><p>In the train() function, I used <em>word_dict</em> and <em>pos_dict</em> to store words and tags but the value of dictionary is index. This is convenient to create related matrix. And I also created a <em>reversed_pos_dict</em> applied to viterbi function. To save memory space, I generated sparsed matrix for emission B matrix, so there was an extra procedure in viterbi function adding k-smooth method on B matrix. </p>
<h3 id="Viterbi-function"><a href="#Viterbi-function" class="headerlink" title="Viterbi function"></a>Viterbi function</h3><p>This function directly deployed viterbi algorithm for a single sentence noted that the input sentence should be a list of word index. The most complex part is recursion step where the algorithm formula is $viterbi[s,t] = \max_{s’=1}^{N} viterbi[s’, t-1] * a_{s’, s}*b_s(O_t)$. However, with broadcasting feature of numpy arrays, the application of this formula in my code is to mutiply viterbi[:, t-1] with shape (s, 1), transition A matrix with shape (s, s) and B($O_s$) with shape (1, s). The result would be a (s, s) matrix where max value of each row is what we need for viterbi[s, t].</p>
<h2 id="K-selection-and-model-evaluation"><a href="#K-selection-and-model-evaluation" class="headerlink" title="K selection and model evaluation"></a>K selection and model evaluation</h2><p>In this case, I continued using grid search to find the best k of smoothing method. Bellowed is grid searching result for k from 0.1 to 1 with step size 0.1 where we can estimate the best k should be less than 0.1.</p>
<p><img src="/2020/04/10/Hidden-Markov/Grid_search_1.png" alt="avatar"></p>
<p>Therefore, I applied a more precise k list from 0.01 to 0.1:</p>
<p><img src="/2020/04/10/Hidden-Markov/Grid_search_2.png" alt="avatar"></p>
<p>The accuracy did not change a lot in different ks, so I chose the best k as 0.08. And the final model accuracy is 0.912123.</p>
]]></content>
      <tags>
        <tag>Hidden Markov Model</tag>
        <tag>NLP</tag>
        <tag>Computational Linguistics</tag>
      </tags>
  </entry>
  <entry>
    <title>N Grams Project</title>
    <url>/2020/03/25/N-Grams-Project/</url>
    <content><![CDATA[<div align="center">

<h1 id="N-gram-Language-Models-and-Evaluation"><a href="#N-gram-Language-Models-and-Evaluation" class="headerlink" title="N-gram Language Models and Evaluation"></a>N-gram Language Models and Evaluation</h1></div>

<h2 id="Program-Instruction"><a href="#Program-Instruction" class="headerlink" title="Program Instruction"></a>Program Instruction</h2><p>Source code can be found on my <a href="https://github.com/GYHenryTT/Computational-Linguistic/tree/master/PA3" target="_blank" rel="noopener">Github</a></p>
<h3 id="1-Bigram-model"><a href="#1-Bigram-model" class="headerlink" title="1. Bigram model"></a>1. Bigram model</h3><p>My bigram model has a different word dictionary from unigram model. I set 0 as the index of STOP word, and for the other words, index is in alphabetical order. In terms of <strong>generateWord(self, context)</strong> function, I used <strong>numpy.random.choice</strong> to generate next word according to the bigram conditional probabilities.</p>
<a id="more"></a>

<h3 id="2-Add-k-smoothing-bigram-model"><a href="#2-Add-k-smoothing-bigram-model" class="headerlink" title="2. Add-k smoothing bigram model"></a>2. Add-k smoothing bigram model</h3><p>In this model, I set k equals to 0.002 which is the best k I got from grid search (mentioned afterwards). To avoid memory error, I kept using sparse matrix to store non-zero word counts. The zero word probability will be processed inside the <strong>getWordProbability</strong> and <strong>generateWord</strong> functions. The rest of code is similar to Bigram model without smoothing.</p>
<h3 id="3-Bigram-model-with-interpolation"><a href="#3-Bigram-model-with-interpolation" class="headerlink" title="3. Bigram model with interpolation"></a>3. Bigram model with interpolation</h3><p>This model combines bigram model without smoothing and Unigram model. Because I had generated different word dictionary in bigram model, in this case, I adjusted the index of Unigram word dictionary (moving the STOP sign to the first of word dictionary).</p>
<h2 id="Result-Evaluation"><a href="#Result-Evaluation" class="headerlink" title="Result Evaluation"></a>Result Evaluation</h2><h3 id="1-Bigram-without-smoothing"><a href="#1-Bigram-without-smoothing" class="headerlink" title="1. Bigram without smoothing"></a>1. Bigram without smoothing</h3><p>The evaluation for bigram model without smoothing is as followed:</p>
<p><img src="/2020/03/25/N-Grams-Project/bigram.png" alt="avatar"></p>
<p>The testing set and jumbled sentences have infinite value of perplexity because the sentence may contain some words with zero conditional probabilities. The training set perplexity is very small (around 81.5), but at the same time it had a low accuracy and high Word Error Rate (WER).</p>
<h3 id="2-Bigram-with-add-k-smoothing"><a href="#2-Bigram-with-add-k-smoothing" class="headerlink" title="2. Bigram with add-k smoothing"></a>2. Bigram with add-k smoothing</h3><p>For k equals to 1, the model gave the following results:</p>
<p><img src="/2020/03/25/N-Grams-Project/bigram_1.png" alt="avatar"></p>
<p>With smoothing, the model had much higher perplexity in training set. This may be due to the effects of the large amount of zero-count words. The model alread had high accuracy and low WER, however, it is worth to find a best k to improve model performance. Accordingly, I made a two-step grid search. First, I set the k equals to [0.0001, 0.001, 0.01, 0.1, 1, 2] in order to have a general image of how model performance changed. The result is shown below:</p>
<p><img src="/2020/03/25/N-Grams-Project/grid_search1.png" alt="avatar"></p>
<p>From the first grid search, I narrowed down my searching scale and continued a more detailed searching with k from the list [0.001 to 0.01]. Finally, I got a dataframe:</p>
<p><img src="/2020/03/25/N-Grams-Project/grid_search2.png" alt="avatar"></p>
<p>The accuracy and WER among these ks have no significant difference. So I chose the k (0.002) with the lowest perplexity in both test set and jumbled sentences. Because the performance did not change a lot, I decided not to go deeper in the grid search. Then, I ran the tester again:</p>
<p><img src="/2020/03/25/N-Grams-Project/bigram_0002.png" alt="avatar"></p>
<p>It got much better performance in perplexity than k = 1 model.</p>
<h3 id="3-Bigram-with-interpolation"><a href="#3-Bigram-with-interpolation" class="headerlink" title="3. Bigram with interpolation"></a>3. Bigram with interpolation</h3><p>Evaluation for this model:</p>
<p><img src="/2020/03/25/N-Grams-Project/bigram_interpolation.png" alt="avatar"></p>
<p>It had the better performance than any other models I got. Instead of EM (expectation-maximization) Algorithm, I applied grid search again to find the best $\lambda$s: </p>
<p><img src="/2020/03/25/N-Grams-Project/grid_search3.png" alt="avatar"><br><img src="/2020/03/25/N-Grams-Project/grid_search4.png" alt="avatar"><br><img src="/2020/03/25/N-Grams-Project/grid_search5.png" alt="avatar"></p>
<p>The best lambda is 0.66 or 0.67. In this case, I chose 0.67 as the best and set this value as default lambda in my code.</p>
]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>Computational Linguistics</tag>
        <tag>N-grams</tag>
      </tags>
  </entry>
  <entry>
    <title>Model Evaluation</title>
    <url>/2020/03/06/Model-Evaluation/</url>
    <content><![CDATA[<h1 id="Model-Evaluation"><a href="#Model-Evaluation" class="headerlink" title="Model Evaluation"></a>Model Evaluation</h1><p>To be continue…</p>
]]></content>
      <categories>
        <category>Learning Journal</category>
      </categories>
      <tags>
        <tag>Classifier Evaluation</tag>
        <tag>Regression Evaluation</tag>
      </tags>
  </entry>
  <entry>
    <title>N-grams and HMM</title>
    <url>/2020/03/06/N-grams-and-HMM/</url>
    <content><![CDATA[<div align="center">

<h1 id="Language-Modeling-and-Sequence-Labeling-N-grams-and-HMM"><a href="#Language-Modeling-and-Sequence-Labeling-N-grams-and-HMM" class="headerlink" title="Language Modeling and Sequence Labeling (N-grams and HMM)"></a>Language Modeling and Sequence Labeling (N-grams and HMM)</h1></div>

<h2 id="Working-Example"><a href="#Working-Example" class="headerlink" title="Working Example"></a>Working Example</h2><h3 id="1-N-grams"><a href="#1-N-grams" class="headerlink" title="1. N-grams"></a>1. N-grams</h3><p>Given the following short sentences:</p>
<pre><code>Alice admired Dorothy 
Dorothy admired every dwarf 
Dorothy cheered 
every dwarf cheered</code></pre><a id="more"></a>

<h4 id="1-1-Train-the-following-n-gram-language-models-on-the-above-data"><a href="#1-1-Train-the-following-n-gram-language-models-on-the-above-data" class="headerlink" title="1.1 Train the following n-gram language models on the above data:"></a>1.1 Train the following n-gram language models on the above data:</h4><p>(a) Unigram, unsmoothed<br>(b) Bigram, unsmoothed<br>(c) Bigram, add-1 smoothing<br>(d) Bigram, interpolation ($λ_1 = λ_2 = 1/2$)</p>
<p>First, create a probability table for unigram model (set unknown word count equal to 1):</p>
<table>
<thead>
<tr>
<th>$w_n$</th>
<th align="right">Alice</th>
<th align="right">admired</th>
<th align="right">Dorothy</th>
<th align="right">every</th>
<th align="right">dwarf</th>
<th align="right">cheered</th>
<th align="right">&lt;/S&gt;</th>
<th align="right">&lt;UNK&gt;</th>
</tr>
</thead>
<tbody><tr>
<td>count</td>
<td align="right">1</td>
<td align="right">2</td>
<td align="right">3</td>
<td align="right">2</td>
<td align="right">2</td>
<td align="right">2</td>
<td align="right">4</td>
<td align="right">1</td>
</tr>
<tr>
<td>$P(w_n)$</td>
<td align="right">0.0588$</td>
<td align="right">0.1176</td>
<td align="right">0.1765</td>
<td align="right">0.1176</td>
<td align="right">0.1176</td>
<td align="right">0.1176</td>
<td align="right">0.2353</td>
<td align="right">0.0588</td>
</tr>
</tbody></table>
<p>Then, create conditional probability tables for bigram model (set unknown word count equal to 1).<br>Here I began with a word count table:<br>$w_n$ | Alice | admired | Dorothy | every | dwarf | cheered | &lt;/S&gt; | &lt;UNK&gt;<br>—|—:|—:|—:|—:|—:|—:|—:|—:<br><strong>&lt;S&gt;</strong> | 1 | 0 | 2 | 1 | 0 | 0 | 0 | 1<br><strong>Alice</strong> | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1<br><strong>admired</strong> | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 1<br><strong>Dorothy</strong> | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 1<br><strong>every</strong> | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 1<br><strong>dwarf</strong> | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1<br><strong>cheered</strong> | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 1<br><strong>&lt;UNK&gt;</strong> | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 </p>
<p>According to this table, the conditional probability table for unsmoothed bigram model would be:<br>$P(w_n |w_{n-1})$ | Alice | admired | Dorothy | every | dwarf | cheered | &lt;/S&gt; | &lt;UNK&gt;<br>—|—:|—:|—:|—:|—:|—:|—:|—:<br><strong>&lt;S&gt;</strong> | 0.2 | 0 | 0.4 | 0.2 | 0 | 0 | 0 | 0.2<br><strong>Alice</strong> | 0 | 0.5 | 0 | 0 | 0 | 0 | 0 | 0.5<br><strong>admired</strong> | 0 | 0 | 0.3333 | 0.3333 | 0 | 0 | 0 | 0.3333<br><strong>Dorothy</strong> | 0 | 0.25 | 0 | 0 | 0 | 0.25 | 0.25 | 0.25<br><strong>every</strong> | 0 | 0 | 0 | 0 | 0.6667 | 0 | 0 | 0.3333<br><strong>dwarf</strong> | 0 | 0 | 0 | 0 | 0 | 0.3333 | 0.3333 | 0.3333<br><strong>cheered</strong> | 0 | 0 | 0 | 0 | 0 | 0 | 0.6667 | 0.3333<br><strong>&lt;UNK&gt;</strong> | 0.125 | 0.125 | 0.125 | 0.125 | 0.125 | 0.125 | 0.125 | 0.125</p>
<p>Using add-1 smoothing, the conditional probability table would be:<br>$w_n$ | Alice | admired | Dorothy | every | dwarf | cheered | &lt;/S&gt; | &lt;UNK&gt;<br>—|—:|—:|—:|—:|—:|—:|—:|—:<br><strong>&lt;S&gt;</strong> | $\frac2{13}=0.1538$ | 0.0769 | 0.2308 | 0.1538 | 0.0769 | 0.0769 | 0.0769 | 0.1538<br><strong>Alice</strong> | 0.1 | 0.2 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.2<br><strong>admired</strong> | 0.0909 | 0.0909 | 0.1818 | 0.1818 | 0.0909 | 0.0909 | 0.0909 | 0.1818<br><strong>Dorothy</strong> | 0.0833 | 0.1667 | 0.0833 | 0.0833 | 0.0833 | 0.1667 | 0.1667 | 0.1667<br><strong>every</strong> | 0.0909 | 0.0909 | 0.0909 | 0.0909 | 0.2727 | 0.0909 | 0.0909 | 0.1818<br><strong>dwarf</strong> | 0.0909 | 0.0909 | 0.0909 | 0.0909 | 0.0909 | 0.1818 | 0.1818 | 0.1818<br><strong>cheered</strong> | 0.0909 | 0.0909 | 0.0909 | 0.0909 | 0.0909 | 0.0909 | 0.2727 | 0.1818<br><strong>&lt;UNK&gt;</strong> | 0.125 | 0.125 | 0.125 | 0.125 | 0.125 | 0.125 | 0.125 | 0.125</p>
<h4 id="1-2-For-each-of-the-above-language-models-compute-the-probability-of-the-following-sentences"><a href="#1-2-For-each-of-the-above-language-models-compute-the-probability-of-the-following-sentences" class="headerlink" title="1.2 For each of the above language models, compute the probability of the following sentences:"></a>1.2 For each of the above language models, compute the probability of the following sentences:</h4><pre><code>Alice cheered
Goldilocks cheered</code></pre><p>(a) Unigram, unsmoothed<br>$$P(Alice \ cheered) = 0.0588 \times 0.1176 = 0.0069$$</p>
<p>$$P(Goldilocks \ cheered) = 0.0588 \times 0.1176 = 0.0069$$</p>
<p>(b) Bigram, unsmoothed</p>
<p>$$P(Alice \ cheered) = 0.2 \times 0 = 0$$</p>
<p>$$P(Goldilocks \ cheered) = 0.2 \times 0.125 = 0.025$$</p>
<p>(c) Bigram, add-1 smoothing</p>
<p>$$P(Alice \ cheered) = 0.1538 \times 0.1 = 0.0154$$</p>
<p>$$P(Goldilocks \ cheered) = 0.1538 \times 0.125 = 0.0192$$</p>
<p>(d) Bigram, interpolation ($λ_1 = λ_2 = 1/2$)</p>
<p>$$P(Alice \ cheered) = (\frac12 \times 0.2 + \frac12 \times 0.0588)(\frac12 \times 0 + \frac12 \times 0.1176) = 0.0076$$</p>
<p>$$P(Goldilocks \ cheered) = (\frac12 \times 0.2 + \frac12 \times 0.0588)(\frac12 \times 0.125 + \frac12 \times 0.1176) = 0.0157$$</p>
<h3 id="2-Hidden-Markov-Models"><a href="#2-Hidden-Markov-Models" class="headerlink" title="2. Hidden Markov Models"></a>2. Hidden Markov Models</h3><p>Given the same short sentences as before, this time tagged with parts of speech:</p>
<pre><code>Alice/NN admired/VB Dorothy/NN 
Dorothy/NN admired/VB every/DT dwarf/NN 
Dorothy/NN cheered/VB 
every/DT dwarf/NN cheered/VB</code></pre><h4 id="2-1-Train-a-hidden-Markov-model-on-the-above-data"><a href="#2-1-Train-a-hidden-Markov-model-on-the-above-data" class="headerlink" title="2.1 Train a hidden Markov model on the above data."></a>2.1 Train a hidden Markov model on the above data.</h4><p>Compute the initial probability distribution $\pi$ with add-1 smoothing:</p>
<table>
<thead>
<tr>
<th>$t_1$</th>
<th align="right">NN</th>
<th align="right">VB</th>
<th align="right">DT</th>
</tr>
</thead>
<tbody><tr>
<td>Count</td>
<td align="right">3+1=4</td>
<td align="right">0+1=1</td>
<td align="right">1+1=2</td>
</tr>
<tr>
<td>$P(t_1)$</td>
<td align="right">0.5714</td>
<td align="right">0.1429</td>
<td align="right">0.2857</td>
</tr>
</tbody></table>
<p>The transition matrix A with add-1 smoothing:</p>
<table>
<thead>
<tr>
<th>Count</th>
<th align="right">NN</th>
<th align="right">VB</th>
<th align="right">DT</th>
</tr>
</thead>
<tbody><tr>
<td>NN</td>
<td align="right">1</td>
<td align="right">5</td>
<td align="right">1</td>
</tr>
<tr>
<td>VB</td>
<td align="right">2</td>
<td align="right">1</td>
<td align="right">2</td>
</tr>
<tr>
<td>DT</td>
<td align="right">3</td>
<td align="right">1</td>
<td align="right">1</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>P($t_n$ | $t_{n-1}$)</th>
<th align="right">NN</th>
<th align="right">VB</th>
<th align="right">DT</th>
</tr>
</thead>
<tbody><tr>
<td>NN</td>
<td align="right">0.1429</td>
<td align="right">0.7143</td>
<td align="right">0.1429</td>
</tr>
<tr>
<td>VB</td>
<td align="right">0.4</td>
<td align="right">0.2</td>
<td align="right">0.4</td>
</tr>
<tr>
<td>DT</td>
<td align="right">0.6</td>
<td align="right">0.2</td>
<td align="right">0.2</td>
</tr>
</tbody></table>
<p>And the emission matrix B with add-1 smoothing:</p>
<table>
<thead>
<tr>
<th>Count</th>
<th align="right">Alice</th>
<th align="right">admired</th>
<th align="right">Dorothy</th>
<th align="right">every</th>
<th align="right">dwarf</th>
<th align="right">cheered</th>
<th align="right">&lt;UNK&gt;</th>
</tr>
</thead>
<tbody><tr>
<td>NN</td>
<td align="right">2</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">1</td>
<td align="right">2</td>
</tr>
<tr>
<td>VB</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">2</td>
</tr>
<tr>
<td>DT</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">2</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>P($w_n$ | $t_n$)</th>
<th align="right">Alice</th>
<th align="right">admired</th>
<th align="right">Dorothy</th>
<th align="right">every</th>
<th align="right">dwarf</th>
<th align="right">cheered</th>
<th align="right">&lt;UNK&gt;</th>
</tr>
</thead>
<tbody><tr>
<td>NN</td>
<td align="right">0.1429</td>
<td align="right">0.0714</td>
<td align="right">0.2857</td>
<td align="right">0.0714</td>
<td align="right">0.2143</td>
<td align="right">0.0714</td>
<td align="right">0.1429</td>
</tr>
<tr>
<td>VB</td>
<td align="right">0.0833</td>
<td align="right">0.25</td>
<td align="right">0.0833</td>
<td align="right">0.0833</td>
<td align="right">0.0833</td>
<td align="right">0.25</td>
<td align="right">0.1667</td>
</tr>
<tr>
<td>DT</td>
<td align="right">0.1</td>
<td align="right">0.1</td>
<td align="right">0.1</td>
<td align="right">0.3</td>
<td align="right">0.1</td>
<td align="right">0.1</td>
<td align="right">0.2</td>
</tr>
</tbody></table>
<h4 id="2-2-Use-the-forward-algorithm-to-compute-the-probability-of-the-following-sentence"><a href="#2-2-Use-the-forward-algorithm-to-compute-the-probability-of-the-following-sentence" class="headerlink" title="2.2 Use the forward algorithm to compute the probability of the following sentence:"></a>2.2 Use the forward algorithm to compute the probability of the following sentence:</h4><pre><code>Alice cheered </code></pre><p>With forward algorithm:</p>
<p>$$\alpha_1(j)=\pi_jb_j(o_1)$$</p>
<p>$$\alpha_t(j)=\sum_{i=1}^N \alpha_{t-1}(i)a_{ij}b_j(o_t)$$</p>
<p>$$P(O|\lambda) = \sum_{i=1}^N \alpha_T(i)$$</p>
<p>Fill in the forward trellis below:</p>
<table>
<thead>
<tr>
<th>$\alpha_t(j)$</th>
<th align="right">Alice</th>
<th align="right">cheered</th>
</tr>
</thead>
<tbody><tr>
<td>NN</td>
<td align="right">$0.5714 \times 0.1429=0.08165$</td>
<td align="right">0.002397</td>
</tr>
<tr>
<td>VB</td>
<td align="right">$0.1429 \times 0.0833=0.0119$</td>
<td align="right">0.01660</td>
</tr>
<tr>
<td>DT</td>
<td align="right">$0.2857 \times 0.1=0.02857$</td>
<td align="right">0.002214</td>
</tr>
</tbody></table>
<p>Note:<br>$ \alpha_2(NN) = (0.08165 \times 0.1429 + 0.0119 \times 0.4 + 0.02857 \times 0.6)\times0.0714 = 0.002397$<br>$ \alpha_2(VB) = (0.08165 \times 0.7143 + 0.0119 \times 0.2 + 0.02857 \times 0.2)\times0.25 = 0.01660$<br>$ \alpha_2(DT) = (0.08165 \times 0.1429 + 0.0119 \times 0.4 + 0.02857 \times 0.2)\times0.1 = 0.002214$</p>
<p>Therefore, the probability of the sentence is $P(Alice \ cheered|\lambda) = 0.002397 + 0.01660 + 0.002214 = 0.0212$</p>
<h4 id="2-3-Use-the-Viterbi-algorithm-to-compute-the-best-tag-sequence-for-the-following-sentence"><a href="#2-3-Use-the-Viterbi-algorithm-to-compute-the-best-tag-sequence-for-the-following-sentence" class="headerlink" title="2.3  Use the Viterbi algorithm to compute the best tag sequence for the following sentence:"></a>2.3  Use the Viterbi algorithm to compute the best tag sequence for the following sentence:</h4><pre><code>Goldilocks cheered</code></pre><p>Fill in the Viterbi trellis below:<br>Viterbi | Goldilocks | cheered |<br>— | —: | —: |<br>NN | $0.5714 \times 0.1429=0.08165$ | $max(0.00083, 0.00068, 0.00245)=0.00245$ |<br>VB | $0.1429 \times 0.1667=0.0238$ | $max(0.0146, 0.0012， 0.0029)=0.0146$ |<br>DT | $0.2857 \times 0.2=0.0571$ | $max(0.00117, 0.00095， 0.00114)=0.00117$ | </p>
<p>Example for $Viterbi[s, 2]$:</p>
<p>$Viterbi[NN, 2] = max(0.08165\times0.1429\times0.0714=0.00083, 0.0238\times0.4\times0.0714=0.00068, 0.0571\times0.6\times0.0714=0.00245)$</p>
<p>$backpointer[NN, 2]=argmax(0.00083, 0.00068, 0.00245)=3$</p>
<table>
<thead>
<tr>
<th>backpointer</th>
<th align="right">Goldilocks</th>
<th align="right">cheered</th>
</tr>
</thead>
<tbody><tr>
<td>NN</td>
<td align="right">0</td>
<td align="right">3 (DT)</td>
</tr>
<tr>
<td>VB</td>
<td align="right">0</td>
<td align="right">1 (NN)</td>
</tr>
<tr>
<td>DT</td>
<td align="right">0</td>
<td align="right">1 (NN)</td>
</tr>
</tbody></table>
<p>Therefore, from the Viterbi trellis the best path probability at time 2 (cheered) is 0.0146. Accordingly, from the backpointer table the best tag sequence in forward order is [NN, VB]</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="noopener">Speech and Language Processing (3rd ed. draft), Dan Jurafsky and James H. Martin</a></p>
]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>Computational Linguistics</tag>
        <tag>N-grams</tag>
        <tag>Hidden Markov Models</tag>
      </tags>
  </entry>
  <entry>
    <title>Naive Bayes Classifier</title>
    <url>/2020/03/01/Naive-Bayes-Classifier/</url>
    <content><![CDATA[<div align="center">

<h1 id="Naive-Bayes-Classifier-and-Evaluation"><a href="#Naive-Bayes-Classifier-and-Evaluation" class="headerlink" title="Naive Bayes Classifier and Evaluation"></a>Naive Bayes Classifier and Evaluation</h1></div>

<h2 id="Program-Instruction"><a href="#Program-Instruction" class="headerlink" title="Program Instruction"></a>Program Instruction</h2><p>The source code has been posted in my <a href="https://github.com/GYHenryTT/Computational-Linguistic/tree/master/PA2" target="_blank" rel="noopener">github</a></p>
<p>Basic Naive Bayes function: $$P(C|W)=\frac{P(W|C)P(C)}{P(W)}$$<br>$$P(C|W) \propto P(W|C)P(C),$$</p>
<p>where $P(W|C)$ is called as likelihood and  $P(C)$ as prior.</p>
<h3 id="1-Training-Function"><a href="#1-Training-Function" class="headerlink" title="1. Training Function"></a>1. Training Function</h3><p>To collect the word counts, we need to first split the text in the training set. In this case, I used regular expression <strong>‘[^A-Za-z\‘]+’</strong> as the seperator. Specifically, it will discard all non-letters except for single quote because some word with quote may have different meanings such as “I’m”, “That’s”, etc.</p>
<p>Then, the class initialization was changed. I used set to collect all the features, and use variable <em>class_count</em> to collect class numbers. Inside the train function, it can visit every seperate word and generate word counts based on the document’s class. Accordingly, it will caculate the priors and likelihoods.</p>
<a id="more"></a>

<h3 id="2-Evaluation-Function"><a href="#2-Evaluation-Function" class="headerlink" title="2. Evaluation Function"></a>2. Evaluation Function</h3><p>This function will generate precision, recall, f1 score and accuracy for both positive class and negative class. If the mode is ‘print’, the function will print out all the results based on the class, otherwise the function will return f1 score for both class as well as accuracy for further analysis (in this case, for grid search, which will be mentioned latter).</p>
<h3 id="3-Feature-selection"><a href="#3-Feature-selection" class="headerlink" title="3. Feature selection"></a>3. Feature selection</h3><p>I applied the mutual information to select features. By this function, I calculated mutual information (also called information gain) for each word in the training set. Afterwards, the function will choose a specific number of the words with top mutual infomation. The number of choosing words would be based on user-defined selection rate (a ratio from 0 to 1).</p>
<h3 id="4-Grid-Search-Function"><a href="#4-Grid-Search-Function" class="headerlink" title="4. Grid Search Function"></a>4. Grid Search Function</h3><p>This is an add-on function for selecting the best feature select rate. This function will take a list of select rates as input. It will use the selected feature to train and test model, print and plot the evaluation results.</p>
<h2 id="Result-Analysis"><a href="#Result-Analysis" class="headerlink" title="Result Analysis"></a>Result Analysis</h2><h3 id="1-Raw-Result"><a href="#1-Raw-Result" class="headerlink" title="1. Raw Result"></a>1. Raw Result</h3><p>At the beginning, I trained the model without feature selection (you may set selection rate as 0 to verify the result). The evaluation result is as followed.</p>
<p><img src="/2020/03/01/Naive-Bayes-Classifier/rawresult.png" alt="avatar"></p>
<p>The raw results are not satisfying, which is why we need to do feature selection.</p>
<h3 id="2-Adjustment-and-Optimization"><a href="#2-Adjustment-and-Optimization" class="headerlink" title="2. Adjustment and Optimization"></a>2. Adjustment and Optimization</h3><p>Then, I implement grid search to find the best select rate and get the following results. These two plots have select rate as X-axis and evaluation result as Y-axis (f1 in both negative and positive class as well as accuracy)</p>
<p><img src="/2020/03/01/Naive-Bayes-Classifier/plot_1.png" alt="avatar"><br><img src="/2020/03/01/Naive-Bayes-Classifier/plot2.png" alt="avatar"></p>
<p>Select rates in the first plot are from 0.01 to 0.99 with step size 0.05, where I found the model perform well in small selection rate. So in the second plot, I chose select rate from 0.001 to 0.1 with step size 0.001. Finally, I got the best selection rate 0.026 which means there will be 1010 words in the selected features. The evaluation of my Naive Bayes model with the best feature set is shown below.</p>
<p><img src="/2020/03/01/Naive-Bayes-Classifier/finalresult.png" alt="avatar"></p>
<p>F1 score for each class and the accuracy have increased by 10 percent based on this feature set. </p>
]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Computational Linguistics</tag>
        <tag>Naive Bayes</tag>
      </tags>
  </entry>
  <entry>
    <title>Naive Bayes and Logistic Regression</title>
    <url>/2020/02/20/Naive-Bayes-and-Logistic-Regression/</url>
    <content><![CDATA[<div align="center">

<h1 id="Naive-Bayes-and-Logistic-Regression-in-Text-Classification"><a href="#Naive-Bayes-and-Logistic-Regression-in-Text-Classification" class="headerlink" title="Naive Bayes and Logistic Regression in Text Classification"></a>Naive Bayes and Logistic Regression in Text Classification</h1></div>

<h2 id="1-Generative-and-Discriminative-Classifiers"><a href="#1-Generative-and-Discriminative-Classifiers" class="headerlink" title="1. Generative and Discriminative Classifiers"></a>1. Generative and Discriminative Classifiers</h2><p>The most important difference between naive Bayes and logistic regression is that logistic regression is a discriminative classifier while naive Bayes is a generative classifier. These are two very different frameworks for how to build a machine learning model.</p>
<a id="more"></a>

<h3 id="1-1-Generative-Classifiers"><a href="#1-1-Generative-Classifiers" class="headerlink" title="1.1 Generative Classifiers"></a>1.1 Generative Classifiers</h3><p>There is a vivid way to understand generative classifier. Consider a visual metaphor: imagine we’re trying to distinguish dog images from cat images. A generative model would have the goal of understanding what dogs look like and what cats look like. Given a test image, the system then asks whether it’s the cat model or the dog model that better fits (is less surprised by) the image, and chooses that as its label.</p>
<p>More formally, a generative model estimates joint distribution $P(c,d)$, where c is the category (class) and d represents observed documents. <strong>Naive Bayes, Hidden Markov Models</strong> and <strong>Bayesian Networks</strong> are widely used generative models.</p>
<h3 id="1-2-Discriminative-Classifiers"><a href="#1-2-Discriminative-Classifiers" class="headerlink" title="1.2 Discriminative Classifiers"></a>1.2 Discriminative Classifiers</h3><p>Back to the visual metaphor, a discriminative model is only trying to learn to distinguish the classes (perhaps without learning much about them). So maybe all the dogs in the training data are wearing collars and the cats aren’t. If that one feature neatly separates the classes, the model is satisfied. If you ask such a model what it knows about cats all it can say is that they don’t wear collars.</p>
<p>Technically, a discriminative model estimates distribution $P(c|d)$ directly. Some popular models are <strong>Logistic Regression, Conditional Random Fields</strong> and <strong>Neural Networks</strong>.</p>
<h2 id="2-Naive-Bayes"><a href="#2-Naive-Bayes" class="headerlink" title="2. Naive Bayes"></a>2. Naive Bayes</h2><p>Naive Bayes is a probabilistic classifier, meaning that for a document d, out of all classes $c\in C$ the classiﬁer returns the class $\hat{c}$ which has the maximum posterior probability given the document. $$\hat{c} = argmaxP(c|d)$$</p>
<p>We use Bayes’ rule to transform this probability: $$P(c|d) = \frac{P(d|c)P(c)}{P(d)}$$</p>
<p>Because for every class, the $P(d)$ will stay the same and not be able to affect $argmax$ function, we are happy to simplify the formula where we have two probabilities: the <strong>prior probability</strong> of the class $P(c)$ and the <strong>likelihood</strong> of the document $P(d|c)$ : $$P(c|d)\propto P(d|c)P(c)$$</p>
<h3 id="2-1-Assumptions"><a href="#2-1-Assumptions" class="headerlink" title="2.1 Assumptions"></a>2.1 Assumptions</h3><p>The function we just mentioned before is still too hard to compute directly. Therefore, Naive Bayes classifiers make two simplifying assumptions.</p>
<p>The first is the <strong>bag of words</strong> assumption discussed intuitively above: we assume the position of words doesn’t matter. Thus we assume that the features $w_1, w_2,…,w_n$ only encode word identity and not position:</p>
<p>$$P(d|c) = P(w_1, w_2,…,w_n|c)$$</p>
<p>The second is commonly called the <strong>naive Bayes assumption</strong>: this is the conditional independence assumption that the probabilities $P(w_i|c)$ are independent given the class c and hence can be ‘naively’ multiplied as follows: </p>
<p>$$P(w_1, w_2,…,w_n|c)=P(w_1|c)P(w_2|c)…P(w_n|c)$$</p>
<h3 id="2-2-Training-algorithm"><a href="#2-2-Training-algorithm" class="headerlink" title="2.2 Training algorithm"></a>2.2 Training algorithm</h3><p><img src="/2020/02/20/Naive-Bayes-and-Logistic-Regression/NB_Algorithm.png" alt="avatar"></p>
<h2 id="3-Logistic-Regression"><a href="#3-Logistic-Regression" class="headerlink" title="3. Logistic Regression"></a>3. Logistic Regression</h2><p>Similar to Naive Bayes, Logistic Regression is a linear classifier. It models P(Y|X) by a linear component and a sigmoid function:</p>
<p>$$P(y=1|x)=\sigma(wx+b)$$</p>
<p>where $$\sigma(z)=\frac{1}{1+e^{-z}}$$</p>
<p>For the second class, $$P(y=0|x)=1-P(y=1|x)$$</p>
<h3 id="3-1-The-cross-entropy-loss-function"><a href="#3-1-The-cross-entropy-loss-function" class="headerlink" title="3.1 The cross-entropy loss function"></a>3.1 The cross-entropy loss function</h3><p>We usually use cross-entropy as the loss function for Logistic Regression. Since there are only two discrete outcomes (1 or 0), we can<br>express the probability that our classifier produces for one observation as the following:</p>
<p>$$P(y|x)=\hat{y}^y(1-\hat{y})^{1-y}$$</p>
<p>Now we take the log of both sides and plug in the definition of $\hat{y}=\sigma(wx+b), $ we will get the cross-entropy loss $L_{CE}$:</p>
<p>$$L_{CE}(w, b)=-[ylog\sigma(wx+b)+(1-y)log(1-\sigma(wx+b))]$$</p>
<h3 id="3-2-Gradient-Descent"><a href="#3-2-Gradient-Descent" class="headerlink" title="3.2 Gradient Descent"></a>3.2 Gradient Descent</h3><p>Our goal with gradient descent is to find the optimal weights: minimize the loss function we’ve defined for the model. For logistic regression, this loss function is conveniently <strong>convex</strong>. A convex function has just one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (By contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima for neural network training and never find the global optimum.)</p>
<p>The following shows the general training steps:</p>
<p><img src="/2020/02/20/Naive-Bayes-and-Logistic-Regression/LR_GD.png" alt="avatar"></p>
<p>However, there are three ways to calculate gradient descent. The <strong>Stochastic Gradient Descent Algorithm</strong> uses the gradient after each sample to update $\theta$. <strong>Batch gradient descent</strong> updates $\theta$ after processing the entire training set. <strong>Minibatch gradient descent</strong> updates $\theta$ after $m$ training examples (Usually we use sum or average of gradients).</p>
<h2 id="4-Worked-Example"><a href="#4-Worked-Example" class="headerlink" title="4. Worked Example"></a>4. Worked Example</h2><p>Assume we have a document set, Given the following short movie reviews, each labeled with a genre, either comedy or action:</p>
<table>
<thead>
<tr>
<th align="center">document</th>
<th align="center">class</th>
</tr>
</thead>
<tbody><tr>
<td align="center">ﬂy fast shoot love</td>
<td align="center">action</td>
</tr>
<tr>
<td align="center">fun couple love love</td>
<td align="center">comedy</td>
</tr>
<tr>
<td align="center">fast furious shoot</td>
<td align="center">action</td>
</tr>
<tr>
<td align="center">couple ﬂy fast fun fun</td>
<td align="center">comedy</td>
</tr>
<tr>
<td align="center">furious shoot shoot fun</td>
<td align="center">action</td>
</tr>
</tbody></table>
<p>and a new document D: [fast couple shoot ﬂy]</p>
<p>compute the most likely class for D.</p>
<h3 id="4-1-Naive-Bayes"><a href="#4-1-Naive-Bayes" class="headerlink" title="4.1 Naive Bayes"></a>4.1 Naive Bayes</h3><p>The generative model for Naive Bayes is $$P(class|feature) = \frac{P(feature|class)P(class)}{P(feature)}$$</p>
<p>Based on the independence assumptions, we will have a caculation formula for the new document “fast couple shoot fly” as </p>
<p>$$P(class|feature) \propto P(fast|class)P(couple|class)P(shoot|class)P(fly|class)P(class)$$</p>
<p>My calculations for the value on the right side are as followed.</p>
<p>$P(comedy)=\frac{n(comedy)}{n(all\ class)}=\frac{2}{5}$<br>$P(action)=\frac{3}{5}$</p>
<p>While the bag of words would be {fly, fast, shoot, love, fun, couple, fast, furious} with length $d=7$, using add-1 smoothing for the likelihoods, I got a conditional probability table,</p>
<table>
<thead>
<tr>
<th align="center">P(feature|class)</th>
<th align="right">comedy</th>
<th align="right">action</th>
</tr>
</thead>
<tbody><tr>
<td align="center">fast</td>
<td align="right">$\frac{n(fast)+1}{words(comedy)+d}=\frac{1+1}{9+7}=\frac{1}{8}$</td>
<td align="right">$\frac{n(fast)+1}{words(action)+d}=\frac{2+1}{11+7}=\frac16$</td>
</tr>
<tr>
<td align="center">couple</td>
<td align="right">$ \frac 3{16}$</td>
<td align="right">$ \frac1{18} $</td>
</tr>
<tr>
<td align="center">shoot</td>
<td align="right">$ \frac1{16}$</td>
<td align="right">$ \frac5{18} $</td>
</tr>
<tr>
<td align="center">fly</td>
<td align="right">$\frac1{8}$</td>
<td align="right">$\frac1{9}$</td>
</tr>
</tbody></table>
<p>Therefore, with the formula mentioned before, the probability of document D belonging to each class will be (use log probability),</p>
<p>$\log{P(comedy|D)} \propto \log(\frac{1}{8} \times \frac{3}{16} \times\frac{1}{16} \times\frac{1}{8} \times \frac{2}{5}) \approx-9.522$<br>$\log{P(action|D)} \propto -8.680 &gt; -9.764$</p>
<p>As a result, the most likely class for D is <strong>action</strong>.</p>
<p>For large dataset, Naive Bayes need feature selection to be more efficient. More information can be accessed in my project <a href="/2020/03/01/Naive-Bayes-Classifier/" title="Naive Bayes Classifier">Naive Bayes Classifier</a></p>
<h3 id="4-2-Logistic-Regression"><a href="#4-2-Logistic-Regression" class="headerlink" title="4.2 Logistic Regression"></a>4.2 Logistic Regression</h3><h4 id="4-2-1-Process-the-first-mini-batch"><a href="#4-2-1-Process-the-first-mini-batch" class="headerlink" title="4.2.1 Process the first mini-batch"></a>4.2.1 Process the first mini-batch</h4><p>Using cross entropy lost function<br>$$L=y\log{\hat{y}} + (1-y)\log{(1-\hat{y})}$$ in this case,  $\hat{y}=\sigma(\theta^TX)$, $\theta=[w_{fast}, w_{couple}, w_{shoot}, w_{fly}, b]$ and $\sigma(z)=\frac1{1+e^{-z}}$</p>
<p>we will get a general gradient formula as<br>$$<br>\nabla L = \begin{bmatrix}<br>[\sigma(\theta^TX)-y]x_{fast}\\<br>[\sigma(\theta^TX)-y]x_{couple}\\<br>[\sigma(\theta^TX)-y]x_{shoot}\\<br>[\sigma(\theta^TX)-y]x_{fly}\\<br>\sigma(\theta^TX)-y<br>\end{bmatrix}<br>$$<br>where $\sigma(\theta^TX) = 0.5$ since we initialized all the weights and bias as zero, so the individual gradiant for the first three examples will be<br>$$<br>\nabla L_{1} = \begin{bmatrix}<br>(0.5 - 0)\times1\\<br>(0.5 - 0)\times0\\<br>(0.5 - 0)\times1\\<br>(0.5 - 0)\times1\\<br>0.5 - 0<br>\end{bmatrix} =<br>\begin{bmatrix}<br>0.5\\<br>0\\<br>0.5\\<br>0.5\\<br>0.5<br>\end{bmatrix}<br>$$<br>$$<br>\nabla L_{2} =<br>\begin{bmatrix}<br>0\\<br>-0.5\\<br>0\\<br>0\\<br>-0.5<br>\end{bmatrix}<br>$$<br>$$<br>\nabla L_{3} =<br>\begin{bmatrix}<br>0.5\\<br>0\\<br>0.5\\<br>0\\<br>0.5<br>\end{bmatrix}<br>$$</p>
<p>The overall gradiant will be</p>
<p>$$<br>\nabla L_{firstbatch} =<br>\begin{bmatrix}<br>\frac13\\<br>-\frac16\\<br>\frac13\\<br>\frac16\\<br>\frac16<br>\end{bmatrix}<br>$$</p>
<p>Use the gradiant to update the weight vector with learning rate $\eta=0.1$</p>
<p>$$<br>\theta_{1} = \theta_{0}-\eta\nabla L_{firstbatch}=<br>\begin{bmatrix}<br>-\frac1{30}\\<br>\frac1{60}\\<br>-\frac1{30}\\<br>-\frac1{60}\\<br>-\frac1{60}<br>\end{bmatrix}<br>$$</p>
<h4 id="4-2-2-Process-the-second-mini-batch"><a href="#4-2-2-Process-the-second-mini-batch" class="headerlink" title="4.2.2 Process the second mini-batch"></a>4.2.2 Process the second mini-batch</h4><p>Caculate individual gradiant in second batch, where $\sigma(\theta_1^TX)=\sigma(1\times(−0.0333) + 1\times(0.0167) + 0\times(−0.0333) + 1\times(−0.0167) − 0.0167)=\sigma(-0.05)=0.4875$</p>
<p>$$<br>\nabla L_{4} =<br>\begin{bmatrix}<br>[\sigma(\theta_1^TX)-1]\times1\\<br>[\sigma(\theta_1^TX)-1]\times1\\<br>[\sigma(\theta_1^TX)-1]\times0\\<br>[\sigma(\theta_1^TX)-1]\times1\\<br>\sigma(\theta_1^TX)-1<br>\end{bmatrix}=<br>\begin{bmatrix}<br>−0.5125\\<br>−0.5125\\<br>0\\<br>−0.5125\\<br>−0.5125<br>\end{bmatrix}<br>$$<br>$$<br>\nabla L_{5} =<br>\begin{bmatrix}<br>[\sigma(\theta_1^TX)-0]\times0\\<br>[\sigma(\theta_1^TX)-0]\times0\\<br>[\sigma(\theta_1^TX)-0]\times2\\<br>[\sigma(\theta_1^TX)-0]\times0\\<br>\sigma(\theta_1^TX)-0<br>\end{bmatrix}=<br>\begin{bmatrix}<br>0\\<br>0\\<br>0.9584\\<br>0\\<br>0.4792<br>\end{bmatrix}<br>$$</p>
<p>The overall gradiant for second batch will be</p>
<p>$$<br>\nabla L_{secondbatch} =<br>\begin{bmatrix}<br>-0.2562\\<br>-0.2562\\<br>0.4792\\<br>-0.2562\\<br>-0.0167<br>\end{bmatrix}<br>$$</p>
<p>Use this gradiant to update the weight vector with learning rate $\eta=0.1$</p>
<p>$$<br>\begin{aligned}<br>\theta_{2} &amp;= \theta_{1}-\eta\nabla L_{secondbatch}\\<br>&amp;=<br>\begin{bmatrix}<br>-\frac1{30}\\<br>\frac1{60}\\<br>-\frac1{30}\\<br>-\frac1{60}\\<br>-\frac1{60}<br>\end{bmatrix}-0.1<br>\begin{bmatrix}<br>-0.2562\\<br>-0.2562\\<br>0.4792\\<br>-0.2562\\<br>-0.0167<br>\end{bmatrix}\\<br>&amp;=<br>\begin{bmatrix}<br>-0.0077\\<br>0.0423\\<br>-0.0813\\<br>0.0090\\<br>-0.0150<br>\end{bmatrix}<br>\end{aligned}<br>$$</p>
<h4 id="4-2-3-Compute-the-probability"><a href="#4-2-3-Compute-the-probability" class="headerlink" title="4.2.3 Compute the probability"></a>4.2.3 Compute the probability</h4><p>The probabability that D has the class “comedy” will be<br>$$\begin{aligned}<br>P(y=1|D)&amp;=\sigma(\theta^TX)\\<br>&amp;=\sigma{(\begin{bmatrix}<br>-0.0077\\<br>0.0423\\<br>-0.0813\\<br>0.0090\\<br>-0.0150<br>\end{bmatrix}^T<br>\begin{bmatrix}<br>1\\<br>1\\<br>1\\<br>1\\<br>1<br>\end{bmatrix})}\\<br>&amp;=0.4868<br>\end{aligned}$$</p>
<p>Therefore, the most likely class for D is <strong>action</strong>.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="noopener">Speech and Language Processing (3rd ed. draft), Dan Jurafsky and James H. Martin</a></p>
]]></content>
      <categories>
        <category>Learning Journal</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Computational Linguistics</tag>
        <tag>Naive Bayes</tag>
        <tag>Logistic Regression</tag>
        <tag>Gradient Descent</tag>
      </tags>
  </entry>
  <entry>
    <title>IS/NIS-DEA Project</title>
    <url>/2018/09/01/IS-DEA-Project/</url>
    <content><![CDATA[<h2 id="IS-NIS-DEA-Project-colleborated-with-Amazon-employee"><a href="#IS-NIS-DEA-Project-colleborated-with-Amazon-employee" class="headerlink" title="IS/NIS-DEA Project colleborated with Amazon employee"></a>IS/NIS-DEA Project colleborated with Amazon employee</h2><p>Use Tableau to analysis delivery miss root cause and apply random forest to forecast ‘obsolete’ sign of products. Source code for random forest and data processing is in my <a href="https://github.com/GYHenryTT/NIS-DEA-Model" target="_blank" rel="noopener">Github</a></p>
<a id="more"></a>



	<div class="row">
    <embed src="Report_of_IS_DEA_Project.pdf" width="100%" height="550" type="application/pdf">
	</div>


]]></content>
      <tags>
        <tag>Amazon</tag>
        <tag>Random Forest</tag>
        <tag>Tableau</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Learning Projects</title>
    <url>/2018/01/11/Deep-Learning-Projects/</url>
    <content><![CDATA[<p>Source codes have been uploaded to my <a href="https://github.com/GYHenryTT/Deep-Learning-Projects" target="_blank" rel="noopener">github</a></p>
<h2 id="1-Convolution-Neural-Networks-CNN"><a href="#1-Convolution-Neural-Networks-CNN" class="headerlink" title="1. Convolution Neural Networks (CNN)"></a>1. Convolution Neural Networks (CNN)</h2><a id="more"></a>



	<div class="row">
    <embed src="Report_CNN.pdf" width="100%" height="550" type="application/pdf">
	</div>




<h2 id="2-Recurrent-Neural-Networks-RNN"><a href="#2-Recurrent-Neural-Networks-RNN" class="headerlink" title="2. Recurrent Neural Networks (RNN)"></a>2. Recurrent Neural Networks (RNN)</h2>

	<div class="row">
    <embed src="Report_RNN.pdf" width="100%" height="550" type="application/pdf">
	</div>




<h2 id="3-Restricted-Boltzmann-Machines-RBM"><a href="#3-Restricted-Boltzmann-Machines-RBM" class="headerlink" title="3. Restricted Boltzmann Machines (RBM)"></a>3. Restricted Boltzmann Machines (RBM)</h2>

	<div class="row">
    <embed src="Report_RBM.pdf" width="100%" height="550" type="application/pdf">
	</div>




<h2 id="4-Reinforcement-Learning"><a href="#4-Reinforcement-Learning" class="headerlink" title="4. Reinforcement Learning"></a>4. Reinforcement Learning</h2>

	<div class="row">
    <embed src="Final_Report.pdf" width="100%" height="550" type="application/pdf">
	</div>


]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>RNN</tag>
        <tag>RBM</tag>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
</search>
