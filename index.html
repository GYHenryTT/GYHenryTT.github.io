<!DOCTYPE html>
<html lang="en">

<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8" />
   
  <meta name="keywords" content="Blog, Code, Learning journal" />
   
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
     Monologue From a Data Geek
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>


  

  

<link rel="alternate" href="/atom.xml" title="Monologue From a Data Geek" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

</html>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="app">
    <main class="content">
      
<section class="cover">
    
      
      <a class="forkMe" href="https://github.com/GYHenryTT"
        target="_blank"><img width="149" height="149" src="/images/forkme.png"
          class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a>
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="/images/cover9.jpg" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">Monologue From a Data Geek</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script>

<div id="main">
  <section class="outer">
  <article class="articles">
    
    
    
    
    <article id="post-Neural-Network-Parsing" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/05/12/Neural-Network-Parsing/"
    >Neural Network Parsing</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/05/12/Neural-Network-Parsing/" class="article-date">
  <time datetime="2020-05-12T04:00:00.000Z" itemprop="datePublished">2020-05-12</time>
</a>
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <div align="center">

<h1 id="Neural-Transition-Based-Dependency-Parsing"><a href="#Neural-Transition-Based-Dependency-Parsing" class="headerlink" title="Neural Transition-Based Dependency Parsing"></a>Neural Transition-Based Dependency Parsing</h1></div>

<h2 id="Worked-Example"><a href="#Worked-Example" class="headerlink" title="Worked Example"></a>Worked Example</h2><h3 id="1-Go-through-the-sequence-of-transitions"><a href="#1-Go-through-the-sequence-of-transitions" class="headerlink" title="1. Go through the sequence of transitions"></a>1. Go through the sequence of transitions</h3><p>Given the sentence:</p>
<pre><code>I parsed this sentence correctly</code></pre><table>
<thead>
<tr>
<th align="left">Stack</th>
<th align="left">Buffer</th>
<th align="left">New dependency</th>
<th align="left">Transition</th>
</tr>
</thead>
<tbody><tr>
<td align="left">[ROOT]</td>
<td align="left">[I, parsed, this, sentence, correctly]</td>
<td align="left"></td>
<td align="left">Initial Configuration</td>
</tr>
<tr>
<td align="left">[ROOT, I]</td>
<td align="left">[parsed, this, sentence, correctly]</td>
<td align="left"></td>
<td align="left">SHIFT</td>
</tr>
<tr>
<td align="left">[ROOT, I, parsed]</td>
<td align="left">[this, sentence, correctly]</td>
<td align="left"></td>
<td align="left">SHIFT</td>
</tr>
<tr>
<td align="left">[ROOT, parsed]</td>
<td align="left">[this, sentence, correctly]</td>
<td align="left">parsed $\rightarrow$ I</td>
<td align="left">LEFT-ARC</td>
</tr>
<tr>
<td align="left">[ROOT, parsed, this]</td>
<td align="left">[sentence, correctly]</td>
<td align="left"></td>
<td align="left">SHIFT</td>
</tr>
<tr>
<td align="left">[ROOT, parsed, this, sentence]</td>
<td align="left">[correctly]</td>
<td align="left"></td>
<td align="left">SHIFT</td>
</tr>
<tr>
<td align="left">[ROOT, parsed, sentence]</td>
<td align="left">[correctly]</td>
<td align="left">sentence $\rightarrow$ this</td>
<td align="left">LEFT-ARC</td>
</tr>
<tr>
<td align="left">[ROOT, parsed]</td>
<td align="left">[correctly]</td>
<td align="left">parsed $\rightarrow$ sentence</td>
<td align="left">RIGHT-ARC</td>
</tr>
<tr>
<td align="left">[ROOT, parsed, correctly]</td>
<td align="left">[]</td>
<td align="left"></td>
<td align="left">SHIFT</td>
</tr>
<tr>
<td align="left">[ROOT, parsed]</td>
<td align="left">[]</td>
<td align="left">parsed $\rightarrow$ correctly</td>
<td align="left">RIGHT-ARC</td>
</tr>
<tr>
<td align="left">[ROOT]</td>
<td align="left">[]</td>
<td align="left">ROOT $\rightarrow$ parsed</td>
<td align="left">RIGHT-ARC</td>
</tr>
</tbody></table>
<h3 id="2-A-sentence-containing-n-words-will-be-parsed-in-how-many-steps"><a href="#2-A-sentence-containing-n-words-will-be-parsed-in-how-many-steps" class="headerlink" title="2. A sentence containing $n$ words will be parsed in how many steps"></a>2. A sentence containing $n$ words will be parsed in how many steps</h3><p>For each word of the sentence, it need first be shifted onto the stack and then reduced by right\left arc. Therefore, there would be $2*n$ parsing steps for a sentence containing $n$ words regardless of the initial configuration.</p>
<h3 id="Check-Dependencies-Error"><a href="#Check-Dependencies-Error" class="headerlink" title="Check Dependencies Error"></a>Check Dependencies Error</h3><p>Dependencies Error:</p>
<pre><code>Prepositional Phrase Attachment Error
Verb Phrase Attachment Error
Modiﬁer Attachment Error
Coordination Attachment Error</code></pre><p>Given four sentences, each one has one dependency error from above. </p>
<p><img src="/2020/05/12/Neural-Network-Parsing/s1.png" alt="avatar"></p>
<p>Error type: Verb Phrase Attachment Error<br>Incorrect dependency: wedding $\rightarrow$ fearing<br>Correct dependency: heading $\rightarrow$ fearing</p>
<p><img src="/2020/05/12/Neural-Network-Parsing/s2.png" alt="avatar"></p>
<p>Error type: Coordination Attachment Error<br>Incorrect dependency: makes $\rightarrow$ rescue<br>Correct dependency: rush $\rightarrow$ rescue</p>
<p><img src="/2020/05/12/Neural-Network-Parsing/s3.png" alt="avatar"></p>
<p>Error type: Prepositional Phrase Attachment Error<br>Incorrect dependency: named $\rightarrow$ Midland<br>Correct dependency: guy $\rightarrow$ Midland</p>
<p><img src="/2020/05/12/Neural-Network-Parsing/s4.png" alt="avatar"></p>
<p>Error type: Modiﬁer Attachment Error<br>Incorrect dependency: elements $\rightarrow$ most<br>Correct dependency: crucial $\rightarrow$ most</p>
<h2 id="Neural-Network-Program"><a href="#Neural-Network-Program" class="headerlink" title="Neural Network Program"></a>Neural Network Program</h2><h3 id="Instruction"><a href="#Instruction" class="headerlink" title="Instruction"></a>Instruction</h3><p>In <em>parser_model.py</em>, weight matrices contain the weight of bias term, so the shape of which is added 1 on rows (first dimension). Also, of the outputs from <em>forward</em> function, the last column (second dimension) of hidden layer outputs is filled with ones representing bias term. </p>
<p>In terms of <em>train_for_epoch</em> in <em>run.py</em>, I take advantage of numpy broadcasting. For each sentence, the backpropagation for all layers is $\nabla_lL=x_l\odot\delta_l$, since we are applying the sum of gradients from one minibatch, my code alters this formula into $\nabla_lL=X_l^T\cdot\Delta_l$, where $X_l$ is the input matrix of current layer with shape of (batch size, input features) and $\Delta_l$ is the error term matrix with shape (batch size, output features). The dot product would be the summation of gradients aggregating by batch size.</p>
<h3 id="Model-Training-and-Evaluation"><a href="#Model-Training-and-Evaluation" class="headerlink" title="Model Training and Evaluation"></a>Model Training and Evaluation</h3><p>With the default hyperparameters (hidden_size=200, lr=0.0005, epoch=10), I got the following results:</p>
<p><img src="/2020/05/12/Neural-Network-Parsing/raw_test.png" alt="avatar"></p>
<p>Then, I implemented a simple grid search on debug mode. First, I changed learning rate to 0.001 to achieve a quick convegence. Accordingly, with differents set of hidden_size and epoch, the grid search results are as bellow:</p>
<table>
<thead>
<tr>
<th>hidden_size/epoch</th>
<th>100</th>
<th>150</th>
<th>200</th>
<th>250</th>
<th>300</th>
<th>400</th>
</tr>
</thead>
<tbody><tr>
<td>10</td>
<td>0.238 <br> 62.22</td>
<td>0.215 <br> 68.63</td>
<td>0.232 <br> 66.95</td>
<td>0.271 <br> 59.91</td>
<td>0.220 <br> 65.02</td>
<td>0.227 <br> 65.24</td>
</tr>
<tr>
<td>20</td>
<td>0.213 <br> 67.21</td>
<td>0.144 <br> 68.84</td>
<td>0.279 <br> 65.99</td>
<td>0.190 <br> 57.46</td>
<td>0.182 <br> 66.09</td>
<td>0.120 <br> 73.86</td>
</tr>
<tr>
<td>30</td>
<td>0.136 <br> 70.85</td>
<td>0.121 <br> 73.91</td>
<td>0.132 <br> 74.96</td>
<td>0.150 <br> 70.57</td>
<td>0.123 <br> 67.50</td>
<td>0.132 <br> 73.04</td>
</tr>
</tbody></table>
<p>The best parameter set is epoch=30, hidden_size=200. After about 20 epoches, the loss and UAS did not change a lot and the result is not much better than 10 epoch</p>
<p><img src="/2020/05/12/Neural-Network-Parsing/result_1.png" alt="avatar"></p>

      
      <!-- reward -->
      
    </div>
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Computational-Linguistics/" rel="tag">Computational Linguistics</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Dependency-Parsing/" rel="tag">Dependency Parsing</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Neural-Network/" rel="tag">Neural Network</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-Distributional-Semantics" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/04/27/Distributional-Semantics/"
    >Distributional Semantics</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/04/27/Distributional-Semantics/" class="article-date">
  <time datetime="2020-04-27T04:00:00.000Z" itemprop="datePublished">2020-04-27</time>
</a>
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <div align="center">

<h1 id="Distributional-Semantics-Takes-the-SAT"><a href="#Distributional-Semantics-Takes-the-SAT" class="headerlink" title="Distributional Semantics Takes the SAT"></a>Distributional Semantics Takes the SAT</h1></div>

<h2 id="1-Create-distributional-semantic-word-vectors"><a href="#1-Create-distributional-semantic-word-vectors" class="headerlink" title="1 Create distributional semantic word vectors"></a>1 Create distributional semantic word vectors</h2><h3 id="Program-Introduction"><a href="#Program-Introduction" class="headerlink" title="Program Introduction"></a>Program Introduction</h3><p>Program details can be seen in Section_1.py. </p>
<p>In this section, I created a class <em>Embedding</em> to complete all the functions needed where <em>word_to_vector</em> function can generate the co-occurrence matrix and PPMI matrix based on the small artificial corpus. Additionally, function <em>get_word_vector</em> is used to print both the count vector and PPMI weighted vector according to the word input. Function <em>euclidean_distance</em> can compute euclidean distance between any two words in the corpus. </p>
<h3 id="Compare-the-word-vector-before-and-after-PPMI-reweighting"><a href="#Compare-the-word-vector-before-and-after-PPMI-reweighting" class="headerlink" title="Compare the word vector before and after PPMI reweighting"></a>Compare the word vector before and after PPMI reweighting</h3><p>The difference between original count vector and PPMI weighted vector for ‘dogs’ is shown below:</p>
<p><img src="/2020/04/27/Distributional-Semantics/dogs_vector_1.png" alt="avatar"></p>
<p>As we can seen in this result, PPMI set 0 value in context words including ‘dogs’, ‘feed’, ‘like’, ‘men’ and ‘women’. These are consistent with count vector (mostly count for 1) except for ‘like’ (count for 11), which makes sense because we are using co-occurrance based on bigrams and from the corpus we could not see many “(‘dogs’, ‘like’)”s. </p>
<p>Most importantly, it is very excitiing to see that word like ‘the’ has the less value (0.87) in PPMI vector even though with very high word counts (91) comparing to ‘bite’ (1.65), because ‘the’ usually could not provide much information in normal corpus. Overall, the PPMI vector is intuitively better than the original count vector by outstanding the obviously related word ‘bite’. </p>
<h3 id="Euclidean-Distance"><a href="#Euclidean-Distance" class="headerlink" title="Euclidean Distance"></a>Euclidean Distance</h3><p>Compute the distance between the following word pairs, note that here I applied PPMI word vector:</p>
<pre><code>“women” and “men” (human noun vs. human noun)
“women” and “dogs” (human noun vs. animal noun)
“men” and “dogs” (human noun vs. animal noun)
“feed” and “like” (human verb vs. human verb)
“feed” and “bite” (human verb vs. animal verb)
“like” and “bite” (human verb vs. animal verb)</code></pre><p>The results is as followed:</p>
<p><img src="/2020/04/27/Distributional-Semantics/eu_distance.png" alt="avatar"></p>
<p>From the table, similar word pairs tend to have less euclidean distance. For example, ‘women’ and ‘men’ has a short distance of 0.68 comparing to ‘men’ and ‘dogs’ with the distance of 2.02. This result confirms our intuition from distributional semantics.</p>
<h3 id="Singular-value-decomposition-SVD"><a href="#Singular-value-decomposition-SVD" class="headerlink" title="Singular-value decomposition (SVD)"></a>Singular-value decomposition (SVD)</h3><p>The following results are PPMI original matrix and recovered matrix, where the two matrix share the same value except for zero values which may be due to the precistion loss of floating-point.</p>
<p><img src="/2020/04/27/Distributional-Semantics/SVD.png" alt="avatar"></p>
<h3 id="Reduced-PPMI-matrix"><a href="#Reduced-PPMI-matrix" class="headerlink" title="Reduced PPMI matrix"></a>Reduced PPMI matrix</h3><p>After reducing the dimensions to 3 on PPMI matrix, I calculated a new euclidean distance table (shown below):</p>
<p><img src="/2020/04/27/Distributional-Semantics/eu_distance_2.png" alt="avatar"></p>
<p>The result keeps the information we need for each word vector because it still confirms our intuition that similar words appear in similar contexts and they have shorter euclidean distance between each other.</p>
<h2 id="2-Computing-with-distributional-semantic-word-vectors"><a href="#2-Computing-with-distributional-semantic-word-vectors" class="headerlink" title="2 Computing with distributional semantic word vectors"></a>2 Computing with distributional semantic word vectors</h2><h3 id="Programming"><a href="#Programming" class="headerlink" title="Programming"></a>Programming</h3><p>Program details can be seen in Section_2.py.</p>
<p>Function <em>create_test_set</em> randomly chose 1000 line from synonymous verbs with replacement, and for each verb, picked 4 random non-synonyms from the data set. Also, set the data set as the answers for multiple-choice questions. This function only ran once since I stored the test set and corrected answers as csv files (test_set.csv, test_correct.csv). Therefore, for the rest of the code, you could simply run <em>load_test_set</em> function to repeat the results. Note that there may be some unknown word in the test set, <em>get_word_vector</em> function could deal with the unknown word by making it a zero array, because the unknown words bear no context from training set so that we could assume they are unrelated to other words. Here I add a really small value (1e-16) on the unknown word vector to avoid error in calculating the cosine similarity. Accordingly, this function will return the word vector. </p>
<p><em>test</em> function collects the accuate numbers of synonymous word predictions including two method, euclidean distance and cosine similarity. To simplify my code, I used cosine distance to represent similarity (cosine distance = 1 - cosine similarity). The prediction word is the one having shortest distance to the base word. </p>
<p>In order to solve SAT Analogy questions, I created a new class inheriting <em>Embedding</em>. I rewrote the <em>create_test_set</em> and <em>test</em> functions to manage the new data set and added a new function <em>create_vector</em> which can help me conveniently create new vectors by any method function.</p>
<h3 id="Synonym-detection"><a href="#Synonym-detection" class="headerlink" title="Synonym detection"></a>Synonym detection</h3><p>The following table contains the accuracy among different set of word vectors and different distance methods:</p>
<p><img src="/2020/04/27/Distributional-Semantics/synonym_test.png" alt="avatar"></p>
<p>It is obvious that word2vec set performs better than COMPOSES and cosine similarity method lead to a higher accuracy. </p>
<h3 id="SAT-Analogy"><a href="#SAT-Analogy" class="headerlink" title="SAT Analogy"></a>SAT Analogy</h3><p>In this case, we need to figure out which word pair is the best match to the based word pair, easy to think about calculating the distance between each word pair and find out the closest one to the distance value of based pair. Intuitively it comes to my mind that I could continue applying cosine distance method based on the word2vec set of word vectors since it performs well in synonym detection. However, after checking the data set, I found cosine similarity may not be applied directly here because it ignores the direction between the two words. For example, in the second question from the data set (listed below), I believed “ostrich bird” and “primate monkey” shared similar cosine distance but it is obviously a wrong answer.</p>
<pre><code>ostrich bird n:n
    lion cat n:n
    goose flock n:n
    ewe sheep n:n
    cub bear n:n
    primate monkey n:n</code></pre><p>So, the direction between two word vectors is important here. Accordingly, I consider the subtraction of two word vectors because it indicates the change (direction) between two vectors explaned by the following figure. After generating this new subtraction vector, I could still use cosine similarity to figure out how close among those new vetors.</p>
<p><img src="/2020/04/27/Distributional-Semantics/subtraction.jpg" alt="avatar"></p>
<p>Therefore, I created subtracted new vector and surprisingly got a high accuracy at 0.438. This exciting finding encouraged me to apply more methods on creating new vectors, such as adding, multiplying, concatenating. And I gained the following results:</p>
<p><img src="/2020/04/27/Distributional-Semantics/analogy.png" alt="avatar"></p>
<p>First of all, these methods all have significant higher accuracy than the random guessing (20%). Another interesting part is that concatenating method also brings a high accuracy (0.422). The reason I think may be that concatenating vectors contain all the information from the paired vectors, concatenating does not change anything on the two vectors, the advantage of which is protecting the information but at the same time the data is kind of redundant. In my opinion, the concatenating method is a good baseline model in analogy. Comparing to concatenating, subtracting, adding and multiplying methods all convert the information to a new form, which intuitively thinking may cause information loss. However, subtraction decribes the direction between two vectors. Because it emphasizes this essential part though losing some other information, we could expect a higher accuracy with this method. The performance of the rest two methods, adding and multiplying, are not very saticfying. This phenomenon is similar to feature reduction or engineering, new information from these methods is not enough to make up for lost information. </p>

      
      <!-- reward -->
      
    </div>
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Co-occurrence-Matrix/" rel="tag">Co-occurrence Matrix</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Computational-Linguistics/" rel="tag">Computational Linguistics</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PPMI-Matrix/" rel="tag">PPMI Matrix</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Synonym-Detection/" rel="tag">Synonym Detection</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-Hidden-Markov" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/04/10/Hidden-Markov/"
    >Hidden Markov</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/04/10/Hidden-Markov/" class="article-date">
  <time datetime="2020-04-10T04:00:00.000Z" itemprop="datePublished">2020-04-10</time>
</a>
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <div align="center">

<h1 id="Part-of-speech-Tagging-with-Hidden-Markov-Models"><a href="#Part-of-speech-Tagging-with-Hidden-Markov-Models" class="headerlink" title="Part-of-speech Tagging with Hidden Markov Models"></a>Part-of-speech Tagging with Hidden Markov Models</h1></div>

<h2 id="Program-Instruction"><a href="#Program-Instruction" class="headerlink" title="Program Instruction"></a>Program Instruction</h2><h3 id="Training-function"><a href="#Training-function" class="headerlink" title="Training function"></a>Training function</h3><p>In the train() function, I used <em>word_dict</em> and <em>pos_dict</em> to store words and tags but the value of dictionary is index. This is convenient to create related matrix. And I also created a <em>reversed_pos_dict</em> applied to viterbi function. To save memory space, I generated sparsed matrix for emission B matrix, so there was an extra procedure in viterbi function adding k-smooth method on B matrix. </p>
<h3 id="Viterbi-function"><a href="#Viterbi-function" class="headerlink" title="Viterbi function"></a>Viterbi function</h3><p>This function directly deployed viterbi algorithm for a single sentence noted that the input sentence should be a list of word index. The most complex part is recursion step where the algorithm formula is $viterbi[s,t] = \max_{s’=1}^{N} viterbi[s’, t-1] * a_{s’, s}*b_s(O_t)$. However, with broadcasting feature of numpy arrays, the application of this formula in my code is to mutiply viterbi[:, t-1] with shape (s, 1), transition A matrix with shape (s, s) and B($O_s$) with shape (1, s). The result would be a (s, s) matrix where max value of each row is what we need for viterbi[s, t].</p>
<h2 id="K-selection-and-model-evaluation"><a href="#K-selection-and-model-evaluation" class="headerlink" title="K selection and model evaluation"></a>K selection and model evaluation</h2><p>In this case, I continued using grid search to find the best k of smoothing method. Bellowed is grid searching result for k from 0.1 to 1 with step size 0.1 where we can estimate the best k should be less than 0.1.</p>
<p><img src="/2020/04/10/Hidden-Markov/Grid_search_1.png" alt="avatar"></p>
<p>Therefore, I applied a more precise k list from 0.01 to 0.1:</p>
<p><img src="/2020/04/10/Hidden-Markov/Grid_search_2.png" alt="avatar"></p>
<p>The accuracy did not change a lot in different ks, so I chose the best k as 0.08. And the final model accuracy is 0.912123.</p>

      
      <!-- reward -->
      
    </div>
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Computational-Linguistics/" rel="tag">Computational Linguistics</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hidden-Markov-Model/" rel="tag">Hidden Markov Model</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-N-Grams-Project" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/25/N-Grams-Project/"
    >N Grams Project</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/25/N-Grams-Project/" class="article-date">
  <time datetime="2020-03-25T04:17:48.000Z" itemprop="datePublished">2020-03-25</time>
</a>
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <div align="center">

<h1 id="N-gram-Language-Models-and-Evaluation"><a href="#N-gram-Language-Models-and-Evaluation" class="headerlink" title="N-gram Language Models and Evaluation"></a>N-gram Language Models and Evaluation</h1></div>

<h2 id="Program-Instruction"><a href="#Program-Instruction" class="headerlink" title="Program Instruction"></a>Program Instruction</h2><p>Source code can be found on my <a href="https://github.com/GYHenryTT/Computational-Linguistic/tree/master/PA3" target="_blank" rel="noopener">Github</a></p>
<h3 id="1-Bigram-model"><a href="#1-Bigram-model" class="headerlink" title="1. Bigram model"></a>1. Bigram model</h3><p>My bigram model has a different word dictionary from unigram model. I set 0 as the index of STOP word, and for the other words, index is in alphabetical order. In terms of <strong>generateWord(self, context)</strong> function, I used <strong>numpy.random.choice</strong> to generate next word according to the bigram conditional probabilities.</p>
      
      <a class="article-more-link" href="/2020/03/25/N-Grams-Project/">Read more</a>
      
      
      <!-- reward -->
      
    </div>
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Computational-Linguistics/" rel="tag">Computational Linguistics</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/N-grams/" rel="tag">N-grams</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-Model-Evaluation" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/06/Model-Evaluation/"
    >Model Evaluation</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/06/Model-Evaluation/" class="article-date">
  <time datetime="2020-03-07T04:19:22.000Z" itemprop="datePublished">2020-03-06</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/Learning-Journal/">Learning Journal</a>
  </div>

      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="Model-Evaluation"><a href="#Model-Evaluation" class="headerlink" title="Model Evaluation"></a>Model Evaluation</h1><p>To be continue…</p>

      
      <!-- reward -->
      
    </div>
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Classifier-Evaluation/" rel="tag">Classifier Evaluation</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Regression-Evaluation/" rel="tag">Regression Evaluation</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-N-grams-and-HMM" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/06/N-grams-and-HMM/"
    >N-grams and HMM</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/06/N-grams-and-HMM/" class="article-date">
  <time datetime="2020-03-06T22:06:23.000Z" itemprop="datePublished">2020-03-06</time>
</a>
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <div align="center">

<h1 id="Language-Modeling-and-Sequence-Labeling-N-grams-and-HMM"><a href="#Language-Modeling-and-Sequence-Labeling-N-grams-and-HMM" class="headerlink" title="Language Modeling and Sequence Labeling (N-grams and HMM)"></a>Language Modeling and Sequence Labeling (N-grams and HMM)</h1></div>

<h2 id="Working-Example"><a href="#Working-Example" class="headerlink" title="Working Example"></a>Working Example</h2><h3 id="1-N-grams"><a href="#1-N-grams" class="headerlink" title="1. N-grams"></a>1. N-grams</h3><p>Given the following short sentences:</p>
<pre><code>Alice admired Dorothy 
Dorothy admired every dwarf 
Dorothy cheered 
every dwarf cheered</code></pre>
      
      <a class="article-more-link" href="/2020/03/06/N-grams-and-HMM/">Read more</a>
      
      
      <!-- reward -->
      
    </div>
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Computational-Linguistics/" rel="tag">Computational Linguistics</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hidden-Markov-Models/" rel="tag">Hidden Markov Models</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/N-grams/" rel="tag">N-grams</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-Naive-Bayes-Classifier" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/01/Naive-Bayes-Classifier/"
    >Naive Bayes Classifier</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/01/Naive-Bayes-Classifier/" class="article-date">
  <time datetime="2020-03-01T22:06:45.000Z" itemprop="datePublished">2020-03-01</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/Project/">Project</a>
  </div>

      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <div align="center">

<h1 id="Naive-Bayes-Classifier-and-Evaluation"><a href="#Naive-Bayes-Classifier-and-Evaluation" class="headerlink" title="Naive Bayes Classifier and Evaluation"></a>Naive Bayes Classifier and Evaluation</h1></div>

<h2 id="Program-Instruction"><a href="#Program-Instruction" class="headerlink" title="Program Instruction"></a>Program Instruction</h2><p>The source code has been posted in my <a href="https://github.com/GYHenryTT/Computational-Linguistic/tree/master/PA2" target="_blank" rel="noopener">github</a></p>
<p>Basic Naive Bayes function: $$P(C|W)=\frac{P(W|C)P(C)}{P(W)}$$<br>$$P(C|W) \propto P(W|C)P(C),$$</p>
<p>where $P(W|C)$ is called as likelihood and  $P(C)$ as prior.</p>
<h3 id="1-Training-Function"><a href="#1-Training-Function" class="headerlink" title="1. Training Function"></a>1. Training Function</h3><p>To collect the word counts, we need to first split the text in the training set. In this case, I used regular expression <strong>‘[^A-Za-z\‘]+’</strong> as the seperator. Specifically, it will discard all non-letters except for single quote because some word with quote may have different meanings such as “I’m”, “That’s”, etc.</p>
<p>Then, the class initialization was changed. I used set to collect all the features, and use variable <em>class_count</em> to collect class numbers. Inside the train function, it can visit every seperate word and generate word counts based on the document’s class. Accordingly, it will caculate the priors and likelihoods.</p>
      
      <a class="article-more-link" href="/2020/03/01/Naive-Bayes-Classifier/">Read more</a>
      
      
      <!-- reward -->
      
    </div>
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Computational-Linguistics/" rel="tag">Computational Linguistics</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Naive-Bayes/" rel="tag">Naive Bayes</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-Naive-Bayes-and-Logistic-Regression" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/02/20/Naive-Bayes-and-Logistic-Regression/"
    >Naive Bayes and Logistic Regression</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/02/20/Naive-Bayes-and-Logistic-Regression/" class="article-date">
  <time datetime="2020-02-20T18:14:09.000Z" itemprop="datePublished">2020-02-20</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/Learning-Journal/">Learning Journal</a>
  </div>

      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <div align="center">

<h1 id="Naive-Bayes-and-Logistic-Regression-in-Text-Classification"><a href="#Naive-Bayes-and-Logistic-Regression-in-Text-Classification" class="headerlink" title="Naive Bayes and Logistic Regression in Text Classification"></a>Naive Bayes and Logistic Regression in Text Classification</h1></div>

<h2 id="1-Generative-and-Discriminative-Classifiers"><a href="#1-Generative-and-Discriminative-Classifiers" class="headerlink" title="1. Generative and Discriminative Classifiers"></a>1. Generative and Discriminative Classifiers</h2><p>The most important difference between naive Bayes and logistic regression is that logistic regression is a discriminative classifier while naive Bayes is a generative classifier. These are two very different frameworks for how to build a machine learning model.</p>
      
      <a class="article-more-link" href="/2020/02/20/Naive-Bayes-and-Logistic-Regression/">Read more</a>
      
      
      <!-- reward -->
      
    </div>
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Computational-Linguistics/" rel="tag">Computational Linguistics</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Gradient-Descent/" rel="tag">Gradient Descent</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Logistic-Regression/" rel="tag">Logistic Regression</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Naive-Bayes/" rel="tag">Naive Bayes</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-IS-DEA-Project" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2018/09/01/IS-DEA-Project/"
    >IS/NIS-DEA Project</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2018/09/01/IS-DEA-Project/" class="article-date">
  <time datetime="2018-09-01T05:51:07.000Z" itemprop="datePublished">2018-09-01</time>
</a>
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="IS-NIS-DEA-Project-colleborated-with-Amazon-employee"><a href="#IS-NIS-DEA-Project-colleborated-with-Amazon-employee" class="headerlink" title="IS/NIS-DEA Project colleborated with Amazon employee"></a>IS/NIS-DEA Project colleborated with Amazon employee</h2><p>Use Tableau to analysis delivery miss root cause and apply random forest to forecast ‘obsolete’ sign of products. Source code for random forest and data processing is in my <a href="https://github.com/GYHenryTT/NIS-DEA-Model" target="_blank" rel="noopener">Github</a></p>
      
      <a class="article-more-link" href="/2018/09/01/IS-DEA-Project/">Read more</a>
      
      
      <!-- reward -->
      
    </div>
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Amazon/" rel="tag">Amazon</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Random-Forest/" rel="tag">Random Forest</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tableau/" rel="tag">Tableau</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-Deep-Learning-Projects" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2018/01/11/Deep-Learning-Projects/"
    >Deep Learning Projects</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2018/01/11/Deep-Learning-Projects/" class="article-date">
  <time datetime="2018-01-12T04:16:28.000Z" itemprop="datePublished">2018-01-11</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/Project/">Project</a>
  </div>

      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>Source codes have been uploaded to my <a href="https://github.com/GYHenryTT/Deep-Learning-Projects" target="_blank" rel="noopener">github</a></p>
<h2 id="1-Convolution-Neural-Networks-CNN"><a href="#1-Convolution-Neural-Networks-CNN" class="headerlink" title="1. Convolution Neural Networks (CNN)"></a>1. Convolution Neural Networks (CNN)</h2>
      
      <a class="article-more-link" href="/2018/01/11/Deep-Learning-Projects/">Read more</a>
      
      
      <!-- reward -->
      
    </div>
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RBM/" rel="tag">RBM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RNN/" rel="tag">RNN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Reinforcement-Learning/" rel="tag">Reinforcement Learning</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
  </article>
  

  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2020
        Henry Gao
      </li>
      <li>
        
          Powered by
        
        
        <a href="https://hexo.io" target="_blank">Hexo</a> Theme <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <span>
  <i>PV:<span id="busuanzi_value_page_pv"></span></i>
  <i>UV:<span id="busuanzi_value_site_uv"></span></i>
</span>
        
      </li>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
    <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
      <aside class="sidebar">
        <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Monologue From a Data Geek"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">Homepage</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">Archives</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">Categories</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">Tags</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">About</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
      </aside>
      <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>Wanna buy me a coffee?</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
  </div>
</div>
      
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<script src="/js/share.js"></script>




<script>
  try {
    var typed = new Typed("#subtitle", {
    strings: ['Henry&#39;s Personal Blog.','Stay hungry. Stay foolish.','Life is not about finding yourself. Life is about creating yourself.'],
    startDelay: 0,
    typeSpeed: 100,
    loop: true,
    backSpeed: 40,
    showCursor: true
    });
  } catch (err) {
  }
  
</script>




<script>
  var ayerConfig = {
    mathjax: true
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>



<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>
  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

</html>