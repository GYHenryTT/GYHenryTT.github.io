<!DOCTYPE html>
<html lang="en">

<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8" />
   
  <meta name="keywords" content="Blog, Code, Learning journal" />
   
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    Naive Bayes and Logistic Regression |  Monologue From a Data Geek
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>


  

  

<link rel="alternate" href="/atom.xml" title="Monologue From a Data Geek" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

</html>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="app">
    <main class="content">
      <section class="outer">
  <article id="post-Naive-Bayes-and-Logistic-Regression" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  Naive Bayes and Logistic Regression
</h1>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/02/20/Naive-Bayes-and-Logistic-Regression/" class="article-date">
  <time datetime="2020-02-20T18:14:09.000Z" itemprop="datePublished">2020-02-20</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/Learning-Journal/">Learning Journal</a>
  </div>

      
    </div>
    

    
    
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <div align="center">

<h1 id="Naive-Bayes-and-Logistic-Regression-in-Text-Classification"><a href="#Naive-Bayes-and-Logistic-Regression-in-Text-Classification" class="headerlink" title="Naive Bayes and Logistic Regression in Text Classification"></a>Naive Bayes and Logistic Regression in Text Classification</h1></div>

<h2 id="1-Generative-and-Discriminative-Classifiers"><a href="#1-Generative-and-Discriminative-Classifiers" class="headerlink" title="1. Generative and Discriminative Classifiers"></a>1. Generative and Discriminative Classifiers</h2><p>The most important difference between naive Bayes and logistic regression is that logistic regression is a discriminative classifier while naive Bayes is a generative classifier. These are two very different frameworks for how to build a machine learning model.</p>
<a id="more"></a>

<h3 id="1-1-Generative-Classifiers"><a href="#1-1-Generative-Classifiers" class="headerlink" title="1.1 Generative Classifiers"></a>1.1 Generative Classifiers</h3><p>There is a vivid way to understand generative classifier. Consider a visual metaphor: imagine we’re trying to distinguish dog images from cat images. A generative model would have the goal of understanding what dogs look like and what cats look like. Given a test image, the system then asks whether it’s the cat model or the dog model that better fits (is less surprised by) the image, and chooses that as its label.</p>
<p>More formally, a generative model estimates joint distribution $P(c,d)$, where c is the category (class) and d represents observed documents. <strong>Naive Bayes, Hidden Markov Models</strong> and <strong>Bayesian Networks</strong> are widely used generative models.</p>
<h3 id="1-2-Discriminative-Classifiers"><a href="#1-2-Discriminative-Classifiers" class="headerlink" title="1.2 Discriminative Classifiers"></a>1.2 Discriminative Classifiers</h3><p>Back to the visual metaphor, a discriminative model is only trying to learn to distinguish the classes (perhaps without learning much about them). So maybe all the dogs in the training data are wearing collars and the cats aren’t. If that one feature neatly separates the classes, the model is satisfied. If you ask such a model what it knows about cats all it can say is that they don’t wear collars.</p>
<p>Technically, a discriminative model estimates distribution $P(c|d)$ directly. Some popular models are <strong>Logistic Regression, Conditional Random Fields</strong> and <strong>Neural Networks</strong>.</p>
<h2 id="2-Naive-Bayes"><a href="#2-Naive-Bayes" class="headerlink" title="2. Naive Bayes"></a>2. Naive Bayes</h2><p>Naive Bayes is a probabilistic classifier, meaning that for a document d, out of all classes $c\in C$ the classiﬁer returns the class $\hat{c}$ which has the maximum posterior probability given the document. $$\hat{c} = argmaxP(c|d)$$</p>
<p>We use Bayes’ rule to transform this probability: $$P(c|d) = \frac{P(d|c)P(c)}{P(d)}$$</p>
<p>Because for every class, the $P(d)$ will stay the same and not be able to affect $argmax$ function, we are happy to simplify the formula where we have two probabilities: the <strong>prior probability</strong> of the class $P(c)$ and the <strong>likelihood</strong> of the document $P(d|c)$ : $$P(c|d)\propto P(d|c)P(c)$$</p>
<h3 id="2-1-Assumptions"><a href="#2-1-Assumptions" class="headerlink" title="2.1 Assumptions"></a>2.1 Assumptions</h3><p>The function we just mentioned before is still too hard to compute directly. Therefore, Naive Bayes classifiers make two simplifying assumptions.</p>
<p>The first is the <strong>bag of words</strong> assumption discussed intuitively above: we assume the position of words doesn’t matter. Thus we assume that the features $w_1, w_2,…,w_n$ only encode word identity and not position:</p>
<p>$$P(d|c) = P(w_1, w_2,…,w_n|c)$$</p>
<p>The second is commonly called the <strong>naive Bayes assumption</strong>: this is the conditional independence assumption that the probabilities $P(w_i|c)$ are independent given the class c and hence can be ‘naively’ multiplied as follows: </p>
<p>$$P(w_1, w_2,…,w_n|c)=P(w_1|c)P(w_2|c)…P(w_n|c)$$</p>
<h3 id="2-2-Training-algorithm"><a href="#2-2-Training-algorithm" class="headerlink" title="2.2 Training algorithm"></a>2.2 Training algorithm</h3><p><img src="/2020/02/20/Naive-Bayes-and-Logistic-Regression/NB_Algorithm.png" alt="avatar"></p>
<h2 id="3-Logistic-Regression"><a href="#3-Logistic-Regression" class="headerlink" title="3. Logistic Regression"></a>3. Logistic Regression</h2><p>Similar to Naive Bayes, Logistic Regression is a linear classifier. It models P(Y|X) by a linear component and a sigmoid function:</p>
<p>$$P(y=1|x)=\sigma(wx+b)$$</p>
<p>where $$\sigma(z)=\frac{1}{1+e^{-z}}$$</p>
<p>For the second class, $$P(y=0|x)=1-P(y=1|x)$$</p>
<h3 id="3-1-The-cross-entropy-loss-function"><a href="#3-1-The-cross-entropy-loss-function" class="headerlink" title="3.1 The cross-entropy loss function"></a>3.1 The cross-entropy loss function</h3><p>We usually use cross-entropy as the loss function for Logistic Regression. Since there are only two discrete outcomes (1 or 0), we can<br>express the probability that our classifier produces for one observation as the following:</p>
<p>$$P(y|x)=\hat{y}^y(1-\hat{y})^{1-y}$$</p>
<p>Now we take the log of both sides and plug in the definition of $\hat{y}=\sigma(wx+b), $ we will get the cross-entropy loss $L_{CE}$:</p>
<p>$$L_{CE}(w, b)=-[ylog\sigma(wx+b)+(1-y)log(1-\sigma(wx+b))]$$</p>
<h3 id="3-2-Gradient-Descent"><a href="#3-2-Gradient-Descent" class="headerlink" title="3.2 Gradient Descent"></a>3.2 Gradient Descent</h3><p>Our goal with gradient descent is to find the optimal weights: minimize the loss function we’ve defined for the model. For logistic regression, this loss function is conveniently <strong>convex</strong>. A convex function has just one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (By contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima for neural network training and never find the global optimum.)</p>
<p>The following shows the general training steps:</p>
<p><img src="/2020/02/20/Naive-Bayes-and-Logistic-Regression/LR_GD.png" alt="avatar"></p>
<p>However, there are three ways to calculate gradient descent. The <strong>Stochastic Gradient Descent Algorithm</strong> uses the gradient after each sample to update $\theta$. <strong>Batch gradient descent</strong> updates $\theta$ after processing the entire training set. <strong>Minibatch gradient descent</strong> updates $\theta$ after $m$ training examples (Usually we use sum or average of gradients).</p>
<h2 id="4-Worked-Example"><a href="#4-Worked-Example" class="headerlink" title="4. Worked Example"></a>4. Worked Example</h2><p>Assume we have a document set, Given the following short movie reviews, each labeled with a genre, either comedy or action:</p>
<table>
<thead>
<tr>
<th align="center">document</th>
<th align="center">class</th>
</tr>
</thead>
<tbody><tr>
<td align="center">ﬂy fast shoot love</td>
<td align="center">action</td>
</tr>
<tr>
<td align="center">fun couple love love</td>
<td align="center">comedy</td>
</tr>
<tr>
<td align="center">fast furious shoot</td>
<td align="center">action</td>
</tr>
<tr>
<td align="center">couple ﬂy fast fun fun</td>
<td align="center">comedy</td>
</tr>
<tr>
<td align="center">furious shoot shoot fun</td>
<td align="center">action</td>
</tr>
</tbody></table>
<p>and a new document D: [fast couple shoot ﬂy]</p>
<p>compute the most likely class for D.</p>
<h3 id="4-1-Naive-Bayes"><a href="#4-1-Naive-Bayes" class="headerlink" title="4.1 Naive Bayes"></a>4.1 Naive Bayes</h3><p>The generative model for Naive Bayes is $$P(class|feature) = \frac{P(feature|class)P(class)}{P(feature)}$$</p>
<p>Based on the independence assumptions, we will have a caculation formula for the new document “fast couple shoot fly” as </p>
<p>$$P(class|feature) \propto P(fast|class)P(couple|class)P(shoot|class)P(fly|class)P(class)$$</p>
<p>My calculations for the value on the right side are as followed.</p>
<p>$P(comedy)=\frac{n(comedy)}{n(all\ class)}=\frac{2}{5}$<br>$P(action)=\frac{3}{5}$</p>
<p>While the bag of words would be {fly, fast, shoot, love, fun, couple, fast, furious} with length $d=7$, using add-1 smoothing for the likelihoods, I got a conditional probability table,</p>
<table>
<thead>
<tr>
<th align="center">P(feature|class)</th>
<th align="right">comedy</th>
<th align="right">action</th>
</tr>
</thead>
<tbody><tr>
<td align="center">fast</td>
<td align="right">$\frac{n(fast)+1}{words(comedy)+d}=\frac{1+1}{9+7}=\frac{1}{8}$</td>
<td align="right">$\frac{n(fast)+1}{words(action)+d}=\frac{2+1}{11+7}=\frac16$</td>
</tr>
<tr>
<td align="center">couple</td>
<td align="right">$ \frac 3{16}$</td>
<td align="right">$ \frac1{18} $</td>
</tr>
<tr>
<td align="center">shoot</td>
<td align="right">$ \frac1{16}$</td>
<td align="right">$ \frac5{18} $</td>
</tr>
<tr>
<td align="center">fly</td>
<td align="right">$\frac1{8}$</td>
<td align="right">$\frac1{9}$</td>
</tr>
</tbody></table>
<p>Therefore, with the formula mentioned before, the probability of document D belonging to each class will be (use log probability),</p>
<p>$\log{P(comedy|D)} \propto \log(\frac{1}{8} \times \frac{3}{16} \times\frac{1}{16} \times\frac{1}{8} \times \frac{2}{5}) \approx-9.522$<br>$\log{P(action|D)} \propto -8.680 &gt; -9.764$</p>
<p>As a result, the most likely class for D is <strong>action</strong>.</p>
<p>For large dataset, Naive Bayes need feature selection to be more efficient. More information can be accessed in my project <a href="/2020/03/01/Naive-Bayes-Classifier/" title="Naive Bayes Classifier">Naive Bayes Classifier</a></p>
<h3 id="4-2-Logistic-Regression"><a href="#4-2-Logistic-Regression" class="headerlink" title="4.2 Logistic Regression"></a>4.2 Logistic Regression</h3><h4 id="4-2-1-Process-the-first-mini-batch"><a href="#4-2-1-Process-the-first-mini-batch" class="headerlink" title="4.2.1 Process the first mini-batch"></a>4.2.1 Process the first mini-batch</h4><p>Using cross entropy lost function<br>$$L=y\log{\hat{y}} + (1-y)\log{(1-\hat{y})}$$ in this case,  $\hat{y}=\sigma(\theta^TX)$, $\theta=[w_{fast}, w_{couple}, w_{shoot}, w_{fly}, b]$ and $\sigma(z)=\frac1{1+e^{-z}}$</p>
<p>we will get a general gradient formula as<br>$$<br>\nabla L = \begin{bmatrix}<br>[\sigma(\theta^TX)-y]x_{fast}\\<br>[\sigma(\theta^TX)-y]x_{couple}\\<br>[\sigma(\theta^TX)-y]x_{shoot}\\<br>[\sigma(\theta^TX)-y]x_{fly}\\<br>\sigma(\theta^TX)-y<br>\end{bmatrix}<br>$$<br>where $\sigma(\theta^TX) = 0.5$ since we initialized all the weights and bias as zero, so the individual gradiant for the first three examples will be<br>$$<br>\nabla L_{1} = \begin{bmatrix}<br>(0.5 - 0)\times1\\<br>(0.5 - 0)\times0\\<br>(0.5 - 0)\times1\\<br>(0.5 - 0)\times1\\<br>0.5 - 0<br>\end{bmatrix} =<br>\begin{bmatrix}<br>0.5\\<br>0\\<br>0.5\\<br>0.5\\<br>0.5<br>\end{bmatrix}<br>$$<br>$$<br>\nabla L_{2} =<br>\begin{bmatrix}<br>0\\<br>-0.5\\<br>0\\<br>0\\<br>-0.5<br>\end{bmatrix}<br>$$<br>$$<br>\nabla L_{3} =<br>\begin{bmatrix}<br>0.5\\<br>0\\<br>0.5\\<br>0\\<br>0.5<br>\end{bmatrix}<br>$$</p>
<p>The overall gradiant will be</p>
<p>$$<br>\nabla L_{firstbatch} =<br>\begin{bmatrix}<br>\frac13\\<br>-\frac16\\<br>\frac13\\<br>\frac16\\<br>\frac16<br>\end{bmatrix}<br>$$</p>
<p>Use the gradiant to update the weight vector with learning rate $\eta=0.1$</p>
<p>$$<br>\theta_{1} = \theta_{0}-\eta\nabla L_{firstbatch}=<br>\begin{bmatrix}<br>-\frac1{30}\\<br>\frac1{60}\\<br>-\frac1{30}\\<br>-\frac1{60}\\<br>-\frac1{60}<br>\end{bmatrix}<br>$$</p>
<h4 id="4-2-2-Process-the-second-mini-batch"><a href="#4-2-2-Process-the-second-mini-batch" class="headerlink" title="4.2.2 Process the second mini-batch"></a>4.2.2 Process the second mini-batch</h4><p>Caculate individual gradiant in second batch, where $\sigma(\theta_1^TX)=\sigma(1\times(−0.0333) + 1\times(0.0167) + 0\times(−0.0333) + 1\times(−0.0167) − 0.0167)=\sigma(-0.05)=0.4875$</p>
<p>$$<br>\nabla L_{4} =<br>\begin{bmatrix}<br>[\sigma(\theta_1^TX)-1]\times1\\<br>[\sigma(\theta_1^TX)-1]\times1\\<br>[\sigma(\theta_1^TX)-1]\times0\\<br>[\sigma(\theta_1^TX)-1]\times1\\<br>\sigma(\theta_1^TX)-1<br>\end{bmatrix}=<br>\begin{bmatrix}<br>−0.5125\\<br>−0.5125\\<br>0\\<br>−0.5125\\<br>−0.5125<br>\end{bmatrix}<br>$$<br>$$<br>\nabla L_{5} =<br>\begin{bmatrix}<br>[\sigma(\theta_1^TX)-0]\times0\\<br>[\sigma(\theta_1^TX)-0]\times0\\<br>[\sigma(\theta_1^TX)-0]\times2\\<br>[\sigma(\theta_1^TX)-0]\times0\\<br>\sigma(\theta_1^TX)-0<br>\end{bmatrix}=<br>\begin{bmatrix}<br>0\\<br>0\\<br>0.9584\\<br>0\\<br>0.4792<br>\end{bmatrix}<br>$$</p>
<p>The overall gradiant for second batch will be</p>
<p>$$<br>\nabla L_{secondbatch} =<br>\begin{bmatrix}<br>-0.2562\\<br>-0.2562\\<br>0.4792\\<br>-0.2562\\<br>-0.0167<br>\end{bmatrix}<br>$$</p>
<p>Use this gradiant to update the weight vector with learning rate $\eta=0.1$</p>
<p>$$<br>\begin{aligned}<br>\theta_{2} &amp;= \theta_{1}-\eta\nabla L_{secondbatch}\\<br>&amp;=<br>\begin{bmatrix}<br>-\frac1{30}\\<br>\frac1{60}\\<br>-\frac1{30}\\<br>-\frac1{60}\\<br>-\frac1{60}<br>\end{bmatrix}-0.1<br>\begin{bmatrix}<br>-0.2562\\<br>-0.2562\\<br>0.4792\\<br>-0.2562\\<br>-0.0167<br>\end{bmatrix}\\<br>&amp;=<br>\begin{bmatrix}<br>-0.0077\\<br>0.0423\\<br>-0.0813\\<br>0.0090\\<br>-0.0150<br>\end{bmatrix}<br>\end{aligned}<br>$$</p>
<h4 id="4-2-3-Compute-the-probability"><a href="#4-2-3-Compute-the-probability" class="headerlink" title="4.2.3 Compute the probability"></a>4.2.3 Compute the probability</h4><p>The probabability that D has the class “comedy” will be<br>$$\begin{aligned}<br>P(y=1|D)&amp;=\sigma(\theta^TX)\\<br>&amp;=\sigma{(\begin{bmatrix}<br>-0.0077\\<br>0.0423\\<br>-0.0813\\<br>0.0090\\<br>-0.0150<br>\end{bmatrix}^T<br>\begin{bmatrix}<br>1\\<br>1\\<br>1\\<br>1\\<br>1<br>\end{bmatrix})}\\<br>&amp;=0.4868<br>\end{aligned}$$</p>
<p>Therefore, the most likely class for D is <strong>action</strong>.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="noopener">Speech and Language Processing (3rd ed. draft), Dan Jurafsky and James H. Martin</a></p>

      
      <!-- reward -->
      
    </div>
      <!-- copyright -->
      
    <footer class="article-footer">
      
          
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        share
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=http://yoursite.com/2020/02/20/Naive-Bayes-and-Logistic-Regression/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Computational-Linguistics/" rel="tag">Computational Linguistics</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Gradient-Descent/" rel="tag">Gradient Descent</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Logistic-Regression/" rel="tag">Logistic Regression</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Naive-Bayes/" rel="tag">Naive Bayes</a></li></ul>


    </footer>

  </div>

  
  
  <nav class="article-nav">
    
      <a href="/2020/03/01/Naive-Bayes-Classifier/" class="article-nav-link">
        <strong class="article-nav-caption">Previous post</strong>
        <div class="article-nav-title">
          
            Naive Bayes Classifier
          
        </div>
      </a>
    
    
  </nav>


  

  
  
<!-- valine评论 -->
<div id="vcomments-box">
    <div id="vcomments">
    </div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js'></script>
<script>
    new Valine({
        el: '#vcomments',
        notify: false,
        verify: '',
        app_id: '',
        app_key: '',
        path: window.location.pathname,
        avatar: 'mp',
        placeholder: 'Some comments on my post?',
        recordIP: true
    });
    const infoEle = document.querySelector('#vcomments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
        infoEle.childNodes.forEach(function (item) {
            item.parentNode.removeChild(item);
        });
    }
</script>
<style>
    #vcomments-box {
        padding: 5px 30px;
    }

    @media screen and (max-width: 800px) {
        #vcomments-box {
            padding: 5px 0px;
        }
    }

    #vcomments-box #vcomments {
        background-color: #fff;
    }

    .v .vlist .vcard .vh {
        padding-right: 20px;
    }

    .v .vlist .vcard {
        padding-left: 10px;
    }
</style>

  

  
  
  

</article>
</section>
      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2020
        Henry Gao
      </li>
      <li>
        
          Powered by
        
        
        <a href="https://hexo.io" target="_blank">Hexo</a> Theme <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <span>
  <i>PV:<span id="busuanzi_value_page_pv"></span></i>
  <i>UV:<span id="busuanzi_value_site_uv"></span></i>
</span>
        
      </li>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
    <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
      <aside class="sidebar">
        <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Monologue From a Data Geek"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">Homepage</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">Archives</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">Categories</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">Tags</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">About</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
      </aside>
      <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>Wanna buy me a coffee?</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
  </div>
</div>
      
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<script src="/js/share.js"></script>




<script>
  try {
    var typed = new Typed("#subtitle", {
    strings: ['Henry&#39;s Personal Blog.','Stay hungry. Stay foolish.','Life is not about finding yourself. Life is about creating yourself.'],
    startDelay: 0,
    typeSpeed: 100,
    loop: true,
    backSpeed: 40,
    showCursor: true
    });
  } catch (err) {
  }
  
</script>




<script src="/js/tocbot.min.js"></script>

<script>
  // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer:'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto',
    onClick: (e) => {
      $('.toc-link').removeClass('is-active-link');
      $(`a[href=${e.target.hash}]`).addClass('is-active-link');
      $(e.target.hash).scrollIntoView();
      return false;
    }
  });
</script>


<script>
  var ayerConfig = {
    mathjax: true
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>



<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>
  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

</html>